{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import itertools\n",
    "from itertools import cycle\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import helpers.helper as helpers\n",
    "import grading.save_student_results as save_student_results\n",
    "\n",
    "\n",
    "mpl.rc('figure', max_open_warning = 0)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graded Exercise 1\n",
    "\n",
    "**Date: 30.10.2020**\n",
    "\n",
    "Welcome to the first graded exercise.\n",
    "\n",
    "You are asked to fill in the code in a couple of cells throughout the exercise. In the end of each cell, where we ask you to fill in some code, you will notice a call to a function from the `save_student_results` module. This ensures that the body of your function is run with pregenerated data and your current results are saved to a file (which you will eventually submit to the Moodle). The cells are independent of each other and you will receive points for each individual cell. We will not grant partial points within a cell.\n",
    "\n",
    "Before you finish, please make sure to **upload two files to Moodle**:\n",
    "* **graded_exercise_1.ipynb**\n",
    "* **answers_SCIPER.npz (e.g. \"answers_280595.npz\")**\n",
    "\n",
    "Good luck! :-)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Margin Width\n",
    "SVM tries to maximize the margin between classes. When accounting for potential class overlap, this maximization comes at the cost of allowing the misclassification of some of the data points. In the following exercise, we ask you to find the width (or magnitude) of the margin of a linear SVM for given regularization hyper-parameter C. You can use scikit-learn's SVM functions.\n",
    "\n",
    "Hint: You can access the dual coefficients and support vectors after fitting from the SVC object. Check the attributes in this documentation: https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html . Lecture 4 slide 22 shows how to compute distance of a point to the decision boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_margin_width(X,y,C):\n",
    "    '''\n",
    "        X: input data NxD\n",
    "        y: output label N\n",
    "        C: scalar value for the regularization term \n",
    "        \n",
    "        return: a scalar width\n",
    "    '''\n",
    "    #YOUR CODE HERE \n",
    "    \n",
    "    ########\n",
    "    \n",
    "    return width\n",
    "    \n",
    "save_student_results.save_find_margin_width(locals())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2. Classification Metrics\n",
    "$\\renewcommand{\\real}{\\mathbb{R}}$\n",
    "\n",
    "In this exercise we will look at some standard means of assessing the quality of binary classifiers, namely the *ROC curve*.\n",
    "\n",
    "Imagine an apple processing factory which has a very unfortunate location in a war zone. There is a conveyor belt bringing harvested apples inside the factory. However, every now and then, a grenade falls on the conveyor belt. To prevent a disaster in the factory, we deploy an automatic apple/grenade classification system. However, since we are underfunded, we can only afford to measure either the weight or the volume of every item on the conveyor belt (not both at the same time). Therefore, we need to decide if we employ a classifier that takes the weight as input, or one that takes the volume.\n",
    "\n",
    "We are given a training dataset containing $N$ samples of apples and grenades with their weight and volume measurements. Furthermore, we decide that our final classifier will be a simple threshold on either the weight or the volume of each incoming item. That is \n",
    "\n",
    "$$P(x_i == grenade) = \\begin{cases} \n",
    "      1 & f(x_i) \\geq t \\\\\n",
    "      0 & \\text{otherwise} \\\\\n",
    "   \\end{cases},\n",
    "$$\n",
    "\n",
    "where $f(x_i)$ measures either the weight or the volume of the item $x_i$ and $t \\in \\real$ is a selected threshold. The implementation of this classifier which you will be using is provided in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_thresh(X, t):\n",
    "    \"\"\" Classifies 1D data X given a threshold `t`.\n",
    "    \n",
    "    Args:\n",
    "        X (np.array): Data, shape (N, )\n",
    "        t (float): Threshold.\n",
    "        \n",
    "    Returns:\n",
    "        np.array: Predicted labels, shape (N, ).\n",
    "    \"\"\"\n",
    "    assert X.ndim == 1\n",
    "    return (X >= t).astype(np.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us have a look at our dataset. By projecting the data either to the *weight* or *volume* axis, we can immediatelly see that it is not clear at all whether weight or volume is a better indicator of receiving a grenade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = helpers.generate_apple_grenade_data()\n",
    "helpers.plot_apple_grenade_data(X, y, class_thresh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your task will be to evaluate the simple threshold classifier both for weight and volume features individually to decide, which one can detect granades more reliably. To this end, we will use a technique common for evaluating binary classifiers, the *ROC curve*.\n",
    "\n",
    "Let the threshold $t$ vary from the minimum to the maximum value (of either weight or volume) in the whole dataset. Generate the ROC  curve. You can use the helper function *plot_roc_curves*, please study its interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "\n",
    "def true_false_pos_neg(y_gt, y_p):\n",
    "    \"\"\" Computes TP, FP, TN, FN.\n",
    "    \n",
    "    Args:\n",
    "        y_gt (np.array): GT labels, shape (N, ).\n",
    "        y_p (np.array): Pred. labels, shape (N, ).\n",
    "        \n",
    "    Returns:\n",
    "        floats: TP, FP, TN, FN\n",
    "    \"\"\"\n",
    "    assert y_gt.ndim == 1 and y_p.ndim == 1\n",
    "    assert y_gt.shape[0] == y_p.shape[0]\n",
    "    N = y_gt.shape[0]\n",
    "    \n",
    "    #YOUR CODE HERE\n",
    "    ...\n",
    "    \n",
    "    \n",
    "    assert tp + fp + tn + fn == N\n",
    "    return tp, fp, tn, fn\n",
    "\n",
    "#############################################\n",
    "save_student_results.save_true_false_pos_neg(locals())\n",
    "#############################################\n",
    "\n",
    "def tp_rate(tp, fp, tn, fn):\n",
    "    \"\"\" Returns true positive rate.\n",
    "    Note: You may not need to use all the arguments\n",
    "    \n",
    "    Args:\n",
    "        fp (float): number of false positives\n",
    "        tp (float): number of true positives\n",
    "        tn (float): number of true negatives\n",
    "        fn (float): number of false negatives\n",
    "        \n",
    "        \n",
    "    Returns:\n",
    "        tp_rate (float): TP rate.\n",
    "    \"\"\"\n",
    "    return ...\n",
    "\n",
    "#############################################\n",
    "save_student_results.save_tp_rate(locals())\n",
    "#############################################\n",
    "\n",
    "def fp_rate(tp, fp, tn, fn):\n",
    "    \"\"\" Returns false positive rate.\n",
    "    Note: You may not need to use all the arguments\n",
    "    \n",
    "    Args:\n",
    "        fp (float): number of false positives\n",
    "        tp (float): number of true positives\n",
    "        tn (float): number of true negatives\n",
    "        fn (float): number of false negatives\n",
    "        \n",
    "        \n",
    "    Returns:\n",
    "        fp_rate (float): FP rate.\n",
    "    \"\"\"\n",
    "    #YOUR CODE HERE\n",
    "    return ...\n",
    "\n",
    "#############################################\n",
    "save_student_results.save_fp_rate(locals())\n",
    "#############################################\n",
    "\n",
    "def roc_curve(fcl, x, y, ts):\n",
    "    \"\"\" Computes data for ROC curve.\n",
    "    \n",
    "    Args:\n",
    "        fcl (callable): Classification function.\n",
    "        x (np.array): Data, shape (N, ).\n",
    "        y (np.array): Gt labels, shape (N, ).\n",
    "        t (np.array): Thresholds, shape (T, ).\n",
    "        \n",
    "    Returns:\n",
    "        tp_rates (np.array): TP rates, shape (T, ).\n",
    "        fp_rates (np.array): FP rates, shape (T, ).\n",
    "    \"\"\"\n",
    "    #YOUR CODE HERE\n",
    "    ...\n",
    "    return np.array(tp_rates), np.array(fp_rates)\n",
    "\n",
    "steps = 1000\n",
    "\n",
    "#############################################\n",
    "save_student_results.save_roc_curve(locals())\n",
    "#############################################\n",
    "\n",
    "### ROC curve\n",
    "ts1 = np.linspace(np.min(X[:, 0]), np.max(X[:, 0]), steps)\n",
    "ts2 = np.linspace(np.min(X[:, 1]), np.max(X[:, 1]), steps)\n",
    "\n",
    "helpers.plot_roc_curves(\n",
    "    *roc_curve(class_thresh, X[:, 0], y, ts1)[::-1], \n",
    "    *roc_curve(class_thresh, X[:, 1], y, ts2)[::-1])\n",
    "\n",
    "#############################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the curve, decide which classifier is better and write its name in the variable *final_classifier* below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "# YOUR CODE HERE\n",
    "\n",
    "final_classifier = ... # 'weight' or 'volume'\n",
    "\n",
    "#############################################\n",
    "save_student_results.save_final_classifier(locals())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Multiclass Logistic Regression\n",
    "$\\renewcommand{\\real}{\\mathbb{R}}$\n",
    "$\\renewcommand{\\nonnegint}{\\mathbb{N_{0}}}$\n",
    "$\\renewcommand{\\xb}{\\mathbf{x}}$\n",
    "$\\renewcommand{\\wb}{\\mathbf{w}}$\n",
    "$\\renewcommand{\\Xb}{\\mathbf{X}}$\n",
    "$\\renewcommand{\\yb}{\\mathbf{y}}$\n",
    "$\\renewcommand{\\Yb}{\\mathbf{Y}}$\n",
    "$\\DeclareMathOperator*{\\argmin}{argmin}$\n",
    "\n",
    "In exercise session 4 you implemented binary logisitc regression. In this exercise we will extend the idea to a logistic regression classifier operating on multiple classes.\n",
    "\n",
    "Let us consider logistic regression for $C$ classes. The weights are stored in a weight matrix $\\mathbf{W}$, where every column is $\\wb_{(k)}$ for class $k$. Therefore, for every class $k$, we learn a separate $\\wb_{(k)}$ during training. The weight matrix will be of size $((D + 1) \\times C)$, where $D$ is dimension of the input ($2$ in our case).\n",
    "\n",
    "First, let us have a look at the data we are dealing with. We have $N$ samples in $3$ classes. Let $X \\in \\real^{N \\times (D + 1)}$ be the training input matrix, $Y \\in \\nonnegint^{N \\times C}$ be ground-truth labels, and $W \\in \\real^{D \\times C}$ be the weight matrix. Run the cell below to load and plot the data. Each class is denoted by a different marker and color.\n",
    "\n",
    "As you can see, the classes severly overlap, and thus we cannot expect $100 \\%$ accuracy for our trained model, since there is clearly no set of linear decision boundaries which perfectly separates the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Generate data.\n",
    "\n",
    "# Settings.\n",
    "mus = np.array([[0., 0.], [2., 3], [-1., 2.]])\n",
    "# stds = np.array([[0.5, 0.5], [0.25, 1.], [0.1, 0.5]])\n",
    "stds = np.array([[0.75, 0.75], [2.5, 1.0], [0.1, 1.5]])\n",
    "Ns = np.array([100, 150, 200])\n",
    "C = Ns.shape[0]\n",
    "\n",
    "# Data.\n",
    "X, Y = helpers.generate_data(mus, stds, Ns, labels_one_hot=True,\n",
    "                     bias=True, shuffle=True)\n",
    "fig = helpers.vis_classes_prediction(X, Y, Y, Ns.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start with implementing the basic building blocks of the multi-class logistic regression that you saw in class, namely\n",
    "\n",
    "- the softmax function\n",
    "- the loss function\n",
    "- the gradient computation\n",
    "- the class prediction function.\n",
    "\n",
    "We prepared the declaration of the corresponding functions below, please implement the bodies of the functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Logistic regression.\n",
    "\n",
    "def softmax(X, W):\n",
    "    \"\"\" Computes softmax.\n",
    "    \n",
    "    Args:\n",
    "        X (np.array): Data, shape (N, D), \n",
    "            N is # samples, D is dimension.\n",
    "        W (np.array): Weights, shape (D, C), \n",
    "            C is # classes.\n",
    "            \n",
    "    Returns:\n",
    "        np.array: Result of shape (N, C).\n",
    "    \"\"\"\n",
    "    \n",
    "    #############################################\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    return ...\n",
    "\n",
    "    #############################################\n",
    "\n",
    "####################\n",
    "save_student_results.save_softmax(locals())\n",
    "####################\n",
    "\n",
    "def loss_logreg(X, Y, W):\n",
    "    \"\"\" Loss function for multi class logistic regression.\n",
    "    \n",
    "    Args:\n",
    "        X (np.array): Input data, shape (N, D).\n",
    "        Y (np.array): GT labels as one-hot, shape (N, C).\n",
    "        W (np.array): Weights, shape (D, C).\n",
    "        \n",
    "    Returns:\n",
    "        float: Loss value.\n",
    "    \"\"\"\n",
    "    #############################################\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    return ...\n",
    "\n",
    "    #############################################\n",
    "    \n",
    "####################\n",
    "save_student_results.save_loss_logreg(locals())\n",
    "####################\n",
    "\n",
    "def gradient_logreg(X, Y, W):\n",
    "    \"\"\" Gradient function for multi class logistic regression.\n",
    "    \n",
    "    Args:\n",
    "        X (np.array): Input data of shape (N, D).\n",
    "        Y (np.array): GT labels as one-hot, shape  (N, ).\n",
    "        W (np.array): Weights, shape (D, C).\n",
    "        \n",
    "    Returns:\n",
    "        np.array: Gradients, shape (D, C).\n",
    "    \"\"\"\n",
    "    \n",
    "    #############################################\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    return ...\n",
    "\n",
    "    #############################################\n",
    "    \n",
    "####################\n",
    "save_student_results.save_gradient_logreg(locals())\n",
    "####################\n",
    "    \n",
    "def predict_logreg(X, W):\n",
    "    \"\"\" Prediction of a class label (index) per data sample.\n",
    "    \n",
    "    Args:\n",
    "        X (np.array): Input data of shape (N, D).\n",
    "        W (np.array): Weights, shape (D, C).\n",
    "        \n",
    "    Returns:\n",
    "        np.array: Predicted labels as indices, (N, ).\n",
    "    \"\"\"\n",
    "    \n",
    "    #############################################\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    return ...\n",
    "\n",
    "    #############################################\n",
    "    \n",
    "####################\n",
    "save_student_results.save_predict_logreg(locals())\n",
    "####################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let us train the logistic regression on the given training dataset. We have provided the training code for you. \n",
    "\n",
    "Upon successful training, you should see an accuracy of $> \\sim85 \\%$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_logreg(X, Y, C, max_iters, lr, print_period=10, plot_period=20):\n",
    "    \"\"\" Training function for multi class logistic regression. \n",
    "    \n",
    "    Args:\n",
    "        X (np.array): Dataset of shape (N, D).\n",
    "        Y (np.array): Labels of shape (N, C).\n",
    "        C (int): Number of classes.\n",
    "        max_iters (integer): Maximum number of iterations.\n",
    "        lr (float): The learning rate of  the gradient step.\n",
    "        print_period (int): Printing period.\n",
    "        plot_period (int): Plotting period.\n",
    "\n",
    "    Returns:\n",
    "        np.array: Learned weights, shape (D, C).\n",
    "    \"\"\"\n",
    "    figs = []\n",
    "    Yinds = helpers.onehot_to_label(Y)\n",
    "\n",
    "    # Initialize weights.\n",
    "    W = np.random.normal(0, 0.1, (X.shape[1], C))  \n",
    "    \n",
    "    # Initial prediction.\n",
    "    acc = helpers.my_accuracy_func(Yinds, predict_logreg(X, W))\n",
    "    figs.append(helpers.vis_classes_prediction(\n",
    "        X, Y, predict_logreg(X, W), C, title=f\"Iter 0, acc: {acc:.2f}\"))\n",
    "    \n",
    "    # Train.    \n",
    "    for it in range(max_iters):\n",
    "\n",
    "        # Compute the gradient and update the weights.\n",
    "        gW = gradient_logreg(X, Y, W)\n",
    "        W -= lr * gW\n",
    "                \n",
    "        if print_period and (it + 1) % print_period == 0:\n",
    "            acc = helpers.my_accuracy_func(Yinds, predict_logreg(X, W))\n",
    "            loss = loss_logreg(X, Y, W)\n",
    "            print(f\"Iter {it + 1} - loss: {loss:.2f}, acc: {acc:.2f}\")\n",
    "        if plot_period and (it + 1) % plot_period == 0:\n",
    "            acc = helpers.my_accuracy_func(Yinds, predict_logreg(X, W))\n",
    "            figs.append(helpers.vis_classes_prediction(\n",
    "                X, Y, predict_logreg(X, W), C, \n",
    "                title=f\"Iter {it + 1}, acc: {acc:.2f}\"))\n",
    "\n",
    "    acc = helpers.my_accuracy_func(Yinds, predict_logreg(X, W))\n",
    "    figs.append(helpers.vis_classes_prediction(\n",
    "        X, Y, predict_logreg(X, W), C, \n",
    "        title=f\"Iter {it + 1}, acc: {acc:.2f}\"))\n",
    "\n",
    "    plt.show()\n",
    "    for f in figs:\n",
    "        plt.close(f)\n",
    "    \n",
    "    return W\n",
    "\n",
    "\n",
    "# Set max iters. and the learning rate.\n",
    "max_iters = 100\n",
    "lr = 0.001\n",
    "\n",
    "W_star = train_logreg(X, Y, C, max_iters, lr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from helpers.helper import plotC, sample_data\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from helpers.helper import load_kernel_dataset_1\n",
    "from helpers.plots import plot\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import tests.tests as tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graded Exercise 2\n",
    "\n",
    "**Date: 06.12.2019**\n",
    "\n",
    "Welcome to the second graded exercise. In this exercise, you will be tested on the following topics: linear classification, linear SVM, PCA, kernel methods. \n",
    "\n",
    "You are asked to fill in the code in a couple of cells throughout the exercise. For each such cell we provided tests which are run along with the cell and save your results to a file. The cells are independent of each other(**otherwise mentioned**) and you will receive points for each individual cell.\n",
    "\n",
    "If your code passes the tests this is a good sign, but it **does not guarantee** that you will get a full grade for that cell.\n",
    "\n",
    "Before you finish, please make sure to **upload two files to Moodle**:\n",
    "* **graded_exercise_2.ipynb**\n",
    "* **answers_SCIPER.npz (e.g. \"answers_280595.npz\")**\n",
    "\n",
    "Good luck! :-)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Important: Enter your SCIPER number below and run the cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sciper_number = 123456  # e.g. 123456"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are dealing with multiclass logistic regression with $K$ classes. We have trained a model and found a set of model weights $\\mathbf{W} \\in \\mathbb{R}^{D \\times K}$, i.e. the weights for all $K$ binary classifiers are stored in columns of $\\mathbf{W}$ so that the weights of the classifier predicting the class with label $0$ are in the 1st column, weights of the classifier predicting the class with label $1$ are in the second column and so on. \n",
    "\n",
    "Given the test dataset $X \\in \\mathbb{R}^{N \\times D}$, for every sample we would like to find the class labels $y \\in \\{0, 1, ..., K-1\\}$ of the $k$ classes with the highest probability as predicted by our model. This is a strategy commonly used in popular classification challenges (such as ImageNet Large Scale Visual Recognition Challenge), where we do not only report our best guess for a given test sample, but also our $k$ best guesses.\n",
    "\n",
    "First fill in the function `find_probabilities` which takes as input test dataset $X$ and model weights $\\mathbf{W}$, and returns an array of shape $(N \\times K)$ as the probabilities of datasamples belonging to each class.\n",
    "\n",
    "Then, fill in the function `top_k` which takes the class probabilities and a number $k$ and returns the labels of $k$ classes with the highest predicted probabilities. The labels must be sorted from the highest to the lowest probability for every data sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_probabilities(X, W):\n",
    "    \"\"\" Given the model represented by the set of weights `W` and the \n",
    "    dataset `X`, this function predicts the probability distribution \n",
    "    of every sample in `X` belonging to a class k.\n",
    "    \n",
    "    Args:\n",
    "        X (np.array): Test dataset, shape (N, D).\n",
    "        W (np.array): Model weights, shape (D, K).\n",
    "        \n",
    "    Returns:\n",
    "        probabilities (np.array of float): Probabilities, shape (N, K).\n",
    "    \"\"\"\n",
    "    ### YOUR CODE HERE ###\n",
    "    XW = \n",
    "    probabilities = ...  \n",
    "    return probabilities\n",
    "    ######################\n",
    "\n",
    "probabilities = tests.test_find_probabilities(locals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_k(probabilities, k):\n",
    "    \"\"\" Given the per-sample probability distribution over labels `probabilities`,\n",
    "    this function selects the `k` highest probability labels for every sample\n",
    "    and returns them in a sorted way.\n",
    "    \n",
    "    Args:\n",
    "        probabilities (np.array): Per-sample probability distribution over labels, \n",
    "            shape (N, K), K is number of classes.\n",
    "        k (int): Number of highest probability classes to return, k < K.\n",
    "        \n",
    "    Returns:\n",
    "        labels (np.array of int): Predicted classes labels sorted by decreasing \n",
    "            probability, shape (N, k).\n",
    "    \"\"\"\n",
    "    ### YOUR CODE HERE ###\n",
    "    labels = ...\n",
    "    return labels\n",
    "    ######################\n",
    "    \n",
    "probabilities = tests.test_top_k(locals())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Count the number of support vectors\n",
    "\n",
    "Note that the decision_function we defined in the function is result of $WX + W_0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Count the number of support vectors\n",
    "def count_support_vectors(decision_function, y):\n",
    "    \"\"\" Given the decision function and labels, output the number of support vectors in \n",
    "    the input dataset.\n",
    "    \n",
    "    Args:\n",
    "        decision_function (np.array): shape (N,), results of wx+w0\n",
    "        y (np.array): shape (N,), binary labels with {-1,1}\n",
    "    \n",
    "    Returns:\n",
    "        count (int): Number of support vectors.\n",
    "    \"\"\"\n",
    "    ### CODE HERE ###\n",
    "    support_vector_indices = ... \n",
    "    count = ...\n",
    "    \n",
    "    return count\n",
    "    \n",
    "tests.test_count_support_vectors(locals())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Compute the primal coefficients\n",
    "\n",
    "Note that the dual coefficients used here are the real ones, not the direct outputs from the fitted model in sklearn.svm.SVC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Compute the primal coefficients\n",
    "\n",
    "def compute_primal_coef(dual_coef, labels, support_vectors):\n",
    "    '''\n",
    "    given real dual coefficients, labels and support_vectors, compute the primal coefficients\n",
    "    \n",
    "    input:\n",
    "    dual_coef: true dual coefficients, np.array of shape (1, n) where n is the number of support vectors.\n",
    "    labels: np.array of shape (1, n) where n is the number of support vectors, the value is either -1 or 1.\n",
    "    support_vectors: np.array of shape (n, D) where n is the number of support vectors and D is the dimension of features.\n",
    "    \n",
    "    output:\n",
    "    primal_coef: primal coefficients, np.array of shape (D, )\n",
    "    '''\n",
    "    ## CODE HERE\n",
    "    primal_coef = ...\n",
    "    \n",
    "    return primal_coef\n",
    "\n",
    "tests.test_compute_primal_coef(locals())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Choose the the approperiate kernel\n",
    "\n",
    "We mainly learn 2 different widely used kernels, they are Polynomial Kernel ('P') and RBF kernels ('R'). \n",
    "\n",
    "Now given a dataset, please compare the models with different kernels and parameters, then choose the approperiate kernel for it.\n",
    "\n",
    "***Question***: Run the 3 cells below to see the visualization of the dataset and classification result using RBF and Polynomial kernels. Looking at the plots, which kernel would you expect to fit the testing data better, given the assumption that testing data holds the same distribution as the training one? (Please choose 'R' or 'P'.) Note that P means Polynomial Kernel and R means RBF kernels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Dataset and visualize it\n",
    "X_1,y_1 = load_kernel_dataset_1()\n",
    "plot(X_1,y_1,None,dataOnly=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_RBF = SVC(kernel = 'rbf', C=1.0, gamma=100.0)\n",
    "\n",
    "## fit the dataset\n",
    "clf_RBF.fit(X_1, y_1)\n",
    "plot(X_1, y_1, clf_RBF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_poly = SVC(kernel='poly', C=1.0, degree=4, gamma=1.0, coef0=1.0)\n",
    "\n",
    "## fit the dataset\n",
    "clf_poly.fit(X_1, y_1)\n",
    "plot(X_1, y_1, clf_poly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ANSWER HERE\n",
    "answer = ''\n",
    "tests.test_choose_kernel(locals(), answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3:  LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Compute Scatter Matrix for LDA\n",
    "In the lecture, you have seen the LDA method. We ask you to implement the two matrices i.e. within-class scatter matrix $\\mathbf{S_w}$ and between-class scatter matrix $\\mathbf{S_b}$, for the given data matrix $\\mathbf{X}$ . In order to compute them we also need data mean $\\bar{\\mathbf{x}}$ and class-wise mean $\\mathbf{\\mu}_c$, which you will implement in the following functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Compute the data mean\n",
    "Input:\n",
    "    X: data matrix of shape NxD\n",
    "Output:\n",
    "    x_bar: mean of entire data of shape (1,D)\n",
    "'''\n",
    "def lda_mat_data_mean(X):\n",
    "    ## CODE HERE\n",
    "    # data mean. Make sure you have proper dimensions of array.\n",
    "    x_bar = ...\n",
    "    return x_bar\n",
    "\n",
    "tests.test_lda_mat_data_mean(locals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Compute the classwise mean\n",
    "Input:\n",
    "    X: data matrix of shape NxD\n",
    "    labels: ground truth class label of shape (N,)\n",
    "    num_classes: number of classes in the dataset\n",
    "Output:\n",
    "    mu_c: list of length num_classes with each element of shape (1,D) array\n",
    "          representing per class mean\n",
    "'''\n",
    "def lda_mat_clswise_mean(X,labels,num_classes):\n",
    "    ## CODE HERE\n",
    "    # class wise mean. Make sure you have proper dimensions of arrays.\n",
    "    mu_c = []\n",
    "    for i in range(num_classes):\n",
    "        mu_c = ...\n",
    "    return mu_c\n",
    "\n",
    "tests.test_lda_mat_clswise_mean(locals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Compute S_w and S_b\n",
    "Input:\n",
    "    X: data matrix of shape NxD\n",
    "    labels: ground truth class label of shape (N,)\n",
    "    num_classes: number of classes in the dataset\n",
    "    x_bar: mean of entire data of shape (1,D)\n",
    "    mu_c: list of length num_classes with each element of shape (1,D) array\n",
    "          representing per class mean\n",
    "Output:\n",
    "    S_w: within-class scatter matrix of shape (D,D)\n",
    "    S_b: between-class scatter matrix of shape (D,D)\n",
    "'''\n",
    "def lda_mat(X,labels,num_classes,x_bar,mu_c):\n",
    "    \n",
    "    ## CODE HERE\n",
    "    # S_w\n",
    "    S_w = np.zeros((X.shape[1],X.shape[1]))\n",
    "    for i in range(num_classes):\n",
    "        x_c = ...\n",
    "        centered_x_c = ...\n",
    "        S_w = ...\n",
    "\n",
    "    ## CODE HERE\n",
    "    # S_b \n",
    "    S_b = np.zeros((X.shape[1],X.shape[1]))\n",
    "    for i in range(num_classes):\n",
    "        N_c = ...\n",
    "        S_b = ...\n",
    "\n",
    "    return S_w, S_b\n",
    "\n",
    "tests.test_lda_mat(locals())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4 Kernel Ridge Regression\n",
    "\n",
    "We are dealing with a kernel ridge regression problem and we are given a training dataset $X \\in \\mathbb{R}^{N \\times D}$. We chose to use a polynomial kernel of degree $3$ and we already computed a kernel matrix $K$.\n",
    "\n",
    "Now we would like to extend our training dataset $X$ by a new **training** sample $x_{\\text{new}} \\in \\mathbb{R}^{D}$. Fill in the function `update_kernel_matrix()` which takes as input the original training dataset $X$, the kernel matrix $K$ precomputed for the training dataset $X$ a new training sample $x_{\\text{new}}$ and returns a new kernel matrix $K_{\\text{new}}$.\n",
    "\n",
    "Below we provide the implementations of polynomial kernel of degree 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kernel_polynomial3(xi, xj):\n",
    "    \"\"\" Computes the polynomial of degree 3 kernel function for the\n",
    "    input vectors `xi`, `xj`.\n",
    "\n",
    "    Args:\n",
    "        xi (np.array): Input vector, shape (D, ).\n",
    "        xj (np.array): Input vector, shape (D, ).\n",
    "\n",
    "    Returns:\n",
    "        float: Result of the kernel function.\n",
    "    \"\"\"\n",
    "    return (xi @ xj + 1) ** 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_kernel_matrix(K, X, x_new):\n",
    "    \"\"\" Updates the kernel matrix `K` given the original\n",
    "    dataset `X` and a new sample `x_new`. Uses kernel function\n",
    "    `kernel_polynomial3`.\n",
    "    \n",
    "    Args:\n",
    "        K (np.array): Kernel matrix.\n",
    "        X (np.array): Original dataset (individual data samples in rows).\n",
    "        x_new (np.array): New data sample.\n",
    "        \n",
    "    Returns:\n",
    "        K_new (np.array): Updated kernel matrix\n",
    "    \"\"\"\n",
    "    ### YOUR CODE HERE ###\n",
    "    \n",
    "    ######################\n",
    "    \n",
    "    return K_new\n",
    "\n",
    "tests.test_kernel_matrix(locals())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

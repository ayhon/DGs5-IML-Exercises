{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import itertools\n",
    "from itertools import cycle\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import helpers.helper as helpers\n",
    "import grading.save_student_results as save_student_results\n",
    "\n",
    "mpl.rc('figure', max_open_warning = 0)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note: Each cell can be evaluated for multiple input cases and the grading will be based on the number of correct outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graded Exercise 1 \n",
    "\n",
    "**Date: 05.11.2021**\n",
    "\n",
    "Welcome to the first graded exercise. In this exercise, you will be tested on the topics of Linear Regression, SVM and KNN.\n",
    "\n",
    "You are asked to fill in the code in a couple of cells throughout the exercise. In the end of each cell where we ask you to fill in some code, you will notice a call to a function from the `save_student_results` module. This ensures that the body of your function is run with pre-generated data and your current results are saved to a file (which you will eventually submit to Moodle). The cells are independent of each other and you will receive points for each individual cell. We will not grant partial points within a cell.\n",
    "\n",
    "Before you finish, please make sure to **upload two files to Moodle**:\n",
    "* **graded_exercise_1.ipynb**\n",
    "* **answers_SCIPER.npz (e.g. \"answers_280595.npz\")**\n",
    "\n",
    "Good luck! :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important: Enter your SCIPER number below and run the cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sciper_number = ...  # e.g. 123456\n",
    "save_student_results.initialize_res(locals())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Linear Regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Introducing the data\n",
    "\n",
    "Noticing the lack of bubble tea on campus, you have decided to open a bubble tea cafe. You will be offering 5 different flavours of tea: black tea, earl gray, jasmine, oolong and hibiscus. Since you want to offer the best possible experience, you will be freshly brewing the tea each morning.\n",
    "\n",
    "Of course, since you are also a machine learning enthusiast, you'd like to have the help of machine learning to know exactly what amounts of tea you should brew for the upcoming day, so that you don't run out and you don't waste tea. Luckily, you have access to a dataset, which documents the amount of tea flavor sold each day. \n",
    "\n",
    "We have our data $\\mathbf{X}$ of shape $N \\times D$, where $N=5000, D=12$  with the following features:\n",
    "- the temperature of the day\n",
    "- the total number of students who have had lunch in the cafeterias\n",
    "- the total number of food trucks on campus\n",
    "- whether it is a vegetarian food day\n",
    "- whether there is a student event on campus or not\n",
    "- total number of times the coffee machines were used on campus\n",
    "- what day of the week it is (spans across 5 features, in one-hot format, e.g., Wednesday corresponds to [0 0 1 0 0] )\n",
    "- bias term\n",
    "\n",
    "Our labels $\\mathbf{Y}$ are of shape $N \\times C$, where $N=5000, C=5$, where each column of $\\mathbf{Y}$ corresponds to the amount of bubble tea sold for each flavor: black tea, earl gray, jasmine, oolong and hibiscus. Let us load this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.load(\"./datasets/linear_reg/bubbletea_data.npy\")\n",
    "Y = np.load(\"./datasets/linear_reg/bubbletea_labels.npy\")\n",
    "print(X.shape, Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We soon receive the news that the dataset contains a mistake: one of the features carries no useful information because it is set to all zeros. Use the following function to detect which feature is problematic (return the index of the feature) and remove the feature from the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_faulty_feature(X):\n",
    "    \"\"\"\n",
    "        Function definition: \n",
    "            Find the faulty feature and return the index. Also return the data with the faulty feature removed\n",
    "        \n",
    "        args:\n",
    "            X (np.array): Data array of shape NXD\n",
    "        returns:\n",
    "            faulty_feature_ind (int): Index of the faulty feature (feature indices start from 0)\n",
    "            X (np.array): Data array of shape NX(D-1)\n",
    "    \"\"\"\n",
    "    faulty_feature_ind = ...\n",
    "    X = ...\n",
    "    return faulty_feature_ind, X\n",
    "\n",
    "save_student_results.save_remove_faulty_feature(locals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faulty_feature_ind, new_X = remove_faulty_feature(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that from now on $D=11$.\n",
    "\n",
    "Now let's split the data into training and test sets (this part has been provided for you.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_ratio = 0.8\n",
    "\n",
    "N = new_X.shape[0]\n",
    "indices = np.arange(N)\n",
    "\n",
    "np.random.seed(42)\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "X_train    = new_X[indices[0:int(N*split_ratio)],:] \n",
    "Y_train    = Y[indices[0:int(N*split_ratio)], :] \n",
    "X_test     = new_X[indices[int(N*(split_ratio)):],:] \n",
    "Y_test     = Y[indices[int(N*(split_ratio)):], :] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Closed-form solution:\n",
    "\n",
    "Let us implement the linear regression model. We will be using the closed-form (analytical) solution seen in class. Reminder: you can use the function `np.linalg.pinv` for the pseudo-inverse! Fill in the function `get_w_analytical` to return the weights of the linear regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_w_analytical(X,Y):\n",
    "    \"\"\"\n",
    "        Function definition:\n",
    "            Find the weights of the linear regression model according to the closed-form solution.\n",
    "        args:\n",
    "            X (np.array): Data array of shape NXD\n",
    "            Y (np.array): Labels array of shape NXC\n",
    "        returns:\n",
    "            weights (np.array): Model parameters of shape DXC\n",
    "    \"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "    w = ...\n",
    "    return w\n",
    "\n",
    "\n",
    "save_student_results.save_get_w_analytical(locals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = get_w_analytical(X_train, Y_train)\n",
    "print(\"Weights have shape\", weights.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Evaluating the model\n",
    "\n",
    "Let's define the RMSE of multiple outputs as\n",
    "\n",
    "$$\n",
    "\\text{RMSE} = \\sqrt{\\frac{1}{C}\\sum_{c=1}^C \\frac{1}{N}\\sum_{n=1}^N (\\mathbf{Y}_{n,c} -  \\hat{\\mathbf{Y}}_{n,c})^2}\n",
    "$$\n",
    "\n",
    "where $\\hat{\\mathbf{Y}}$ are our predictions using the model we just optimized.\n",
    "\n",
    "Compute the RMSE value of the test data. Fill in the function below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RMSE(X, Y, weights):\n",
    "    \"\"\"\n",
    "        Function definition:\n",
    "        \n",
    "        args:\n",
    "            X (np.array): Data array of shape NXD\n",
    "            Y (np.array): Labels array of shape NXC\n",
    "            weights (np.array): Model parameters of shape DXC\n",
    "        returns:\n",
    "            rmse (float): RMSE value \n",
    "    \"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "    \n",
    "    rmse = ...\n",
    "    return rmse\n",
    "\n",
    "save_student_results.save_RMSE(locals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = RMSE(X_test, Y_test, weights)\n",
    "print(\"RMSE is\", rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Analyzing the weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will be analyzing the weights we just optimized. We would like to find which features of the data correlate positively with the different flavors of tea. For instance, we would like to know which features of the data cause higher sales of \"oolong\" bubble tea. Fill in the function below which returns the indices of the features that are positively correlated with higher sales of a specified flavor. \n",
    "\n",
    "Note: we are not interested in the index of the bias feature!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positively_correlated_features(weights, flavor):\n",
    "    \"\"\"\n",
    "        Function definition:\n",
    "            This function helps us analyze the relationship between the data features and the weights. \n",
    "        args:\n",
    "            weights (np.array): Weights matrix of shape DxC\n",
    "            flavor (str): One of either \"blacktea\", \"earlgray\", \"jasmine\", \"oolong\", \"hibiscus\"\n",
    "        returns:\n",
    "            pos_corr_feat_ind (list or np.array): list of indices of features which have a\n",
    "                                                    positive correlation (feature indices start with 0)\n",
    "    \"\"\"\n",
    "    keys = [\"blacktea\", \"earlgray\", \"jasmine\", \"oolong\", \"hibiscus\"]\n",
    "    ### YOUR CODE HERE:\n",
    "\n",
    "    ###\n",
    "    return pos_corr_feat_ind\n",
    "\n",
    "save_student_results.save_positively_correlated_features(locals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_corr_feat_ind = positively_correlated_features(weights, \"hibiscus\")\n",
    "print(\"Positively correlated indices are\", pos_corr_feat_ind)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2. SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Equal distances to the decision boundary\n",
    "As it was discussed during the lectures, different classifiers have different decision boundaries. SVM is designed to find a decision boundary which is as far as possible from the training points of both classes. \n",
    "In this exercise, we propose to verify that this property holds for a trained SVM. Note that we restrict ourselves to the **linearly-separable** scenario. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's denote the \"minimal distance\" to the decision boundary for class $i$ in the case of **linearly-separable** classes as\n",
    "$$ d_i = \\min_{j \\in C_i}{ r_j }$$\n",
    "where $r_j$ is the distance between the sample $x_j$ of class $i$ and the decision boundary, and $C_i$ is the set of the indices of the samples belonging to class $i$.\n",
    "\n",
    "The task of this exercise is to check that these minimal distances for both classes are indeed equal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We provide the data and the model that we used in class. You can use them for debugging your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "# Get the simple dataset\n",
    "X, Y = helpers.get_simple_dataset()\n",
    " \n",
    "# Create an SVM model with a linear kernel and C=1.0\n",
    "clf = SVC(kernel='linear', C=1.)\n",
    "\n",
    "# Train the model\n",
    "clf.fit(X, Y)\n",
    "\n",
    "# Plot its decision boundary\n",
    "helpers.plot(X, Y, clf)\n",
    "\n",
    "# Get the primal coefficients of this trained SVM model\n",
    "w = clf.coef_[0]\n",
    "w0 = clf.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Distance to the decision boundary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a reminder, the absolute (i.e., unsigned) distance to the decision boundary for a data sample $x$ is given by\n",
    "$$r = \\frac{|\\hat{y}(\\mathbf{x})|}{\\|\\tilde{\\mathbf{w}}\\|} = \\frac{|\\tilde{\\mathbf{w}}^T\\mathbf{x} + w^{(0)}|}{\\|\\tilde{\\mathbf{w}}\\|}$$\n",
    "(Note that, for linearly separable data, using the absolute value $|\\hat{y}(\\mathbf{x})|$ is equivalent to multiplying the prediction $\\hat{y}(\\mathbf{x})$ by the true label $y$ as seen in class.) \n",
    "\n",
    "Your first task is therefore to implement the computation of $\\hat{y}(\\mathbf{x}) = \\tilde{\\mathbf{w}}^T\\mathbf{x} + w^{(0)}$ in `decision_function`. Complete the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the weights (w and w0) from the fitted model to predict the decision function value of input data points\n",
    "# Beware: the expected output is an array of floating point values (y_hat), not the predicted class.\n",
    "\n",
    "def decision_function(x, w, w0):\n",
    "    '''\n",
    "    given input data, w and w0, output the VALUE of the decision function (y^hat) for data points x\n",
    "    \n",
    "    input:\n",
    "    x: data, np.array of shape (N, D) where N is the number of datapoints and D is the dimension of features.\n",
    "    w: weights, np.array of shape (D,)\n",
    "    w0: bias, np.array of shape (1,)\n",
    "    \n",
    "    output:\n",
    "    y_hat: decision function values, np.array of shape (N, ). \n",
    "    '''\n",
    "    ## CODE HERE\n",
    "    return ...\n",
    "\n",
    "y_hat = decision_function(X, w, w0)\n",
    "\n",
    "save_student_results.decision_function(locals())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use the above `y_hat` to compute the unsigned distance of the N data points to the decision boundary.\n",
    "\n",
    "*Hint: to compute the norm of a vector you can use* `np.linalg.norm()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist(y_hat, w):\n",
    "    \"\"\"\n",
    "    Computes the minimum distance between the decision boundary and the samples of the class.\n",
    "    \n",
    "    Parameters: \n",
    "        y_hat: np.array of shape (N, ) where N is the number of data samples: values of the decision function for these data samples\n",
    "        w (np.array, shape (D,) ): Primal weights\n",
    "    Returns:\n",
    "        d (N, ): distances of the N data samples to the decision boundary\n",
    "    \"\"\" \n",
    "    ### CODE HERE\n",
    "    d = ...\n",
    "    \n",
    "    return d\n",
    "\n",
    "r = dist(y_hat, w)\n",
    "\n",
    "save_student_results.dist(locals())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Equality of the SVM margins\n",
    "\n",
    "We have implemented above the computation of distances between all data points and the decision boundary. These distances are stored in `r`. We now need to split these distances into 2 arrays: `r_pos` containing the distances of points with ground-truth label +1, and `r_neg` containing distances of points with ground-truth label -1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dists(r, Y):\n",
    "    \"\"\"\n",
    "    Splits array r based on ground truth labels Y\n",
    "    \n",
    "    Parameters: \n",
    "        r (np.array, shape (N,) ): unsigned distance of N data samples to the decision boundary\n",
    "        Y (np.array, shape (N,)): Ground-truth labels (1 or -1) corresponding to every sample in X\n",
    "    Returns: with N=M+L\n",
    "        r_pos (np.array, shape (M,) ): distances to the decision boundary of data samples with label +1\n",
    "        r_neg (np.array, shape (L,) ): distances to the decision boundary of data samples with label -1\n",
    "    \"\"\"\n",
    "    ### CODE HERE\n",
    "    r_pos = ...\n",
    "    r_neg = ...\n",
    "\n",
    "    return r_pos, r_neg\n",
    "\n",
    "r_pos, r_neg = split_dists(r, Y)\n",
    "\n",
    "save_student_results.split_dists(locals())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now check that the minimum distances to the decision boundary for both classes are equal. Since the trained SVM model is the result of a numerical process, small errors can accumulate. We will therefore not actually check for a strict equality but for closeness: we define scalars $a$ and $b$ as being close when\n",
    "$$| a - b | < 10^{-4}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def are_minimum_distances_close(r_pos, r_neg):\n",
    "    \"\"\"\n",
    "    Returns True iff the minimum of r_pos and r_neg are close to each other (up to 1e-4)\n",
    "    \n",
    "    Parameters: \n",
    "        r_pos (np.array, shape (M,) ): distances to the decision boundary of data samples with label +1\n",
    "        r_neg (np.array, shape (L,) ): distances to the decision boundary of data samples with label -1\n",
    "    Returns:\n",
    "        (bool): True iff the minimum of r_pos and r_neg are close to each other (up to 1e-4)\n",
    "    \"\"\" \n",
    "    ### CODE HERE\n",
    "    return ...\n",
    "\n",
    "eq_min_distances = are_minimum_distances_close(r_pos, r_neg)\n",
    "print(f'Distances are equal: {eq_min_distances}')\n",
    "\n",
    "\n",
    "save_student_results.are_minimum_distances_close(locals())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Analyzing slack variables \n",
    "\n",
    "Assume that the SVM model was trained on **non linearly-separable** data this time. Here is the classic (primal) formulation of SVM that was used:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "        \\underset{\\tilde{\\mathbf{w}},w^{(0)},{\\{\\xi_i\\}}}{\\operatorname{min}}  \\ \\ & \\frac{1}{2}\\|\\tilde{\\mathbf{w}}\\|^2 + C \\sum^N_{i=1}\\xi_i \\\\\n",
    "        \\operatorname{subject \\  to} \\ \\ &  y_i(\\tilde{\\mathbf{w}}^T\\mathbf{x_i}+w^{(0)}) \\geq 1-\\xi_i , \\forall \\  i \\\\\n",
    "                          &  \\xi_i \\geq 0 , \\forall \\  i\n",
    "    \\end{align}\n",
    "$$\n",
    "\n",
    "where $\\tilde{\\mathbf{w}}$,$w^{(0(}$ are the weights and the bias, $C$ weighs the penalty term, $\\xi_i$ is a slack variable encoding how far data point $i$ is beyond the correct margin and $y_i \\in\\{-1,1\\}$ is the ground-truth label of sample $i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose that the person who trained the SVM did not properly save the trained weights, but instead saved the values of the slack variables only. As a result, the array of slack variables $\\{\\xi_i\\}$ that correspond to the training samples $\\{x_i\\}$ is the **only** data that is available to you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load slack variables\n",
    "slacks = np.load('./datasets/svm/slack_array.npy')\n",
    "plt.plot(sorted(slacks))\n",
    "print('Number of slack variables: ', slacks.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4.1 Task:\n",
    "Given the array of slack variables, we would like to recover some information about the lost SVM model. You are asked to compute the following statistics based on the slack variables' values:\n",
    "* Accuracy of classification for the training samples $\\Big(\\frac{\\text{# correct predictions}}{\\text{# samples}}\\Big)$ . We consider a point to be correctly classified if it lies **strictly** on the correct size of the decision boundary (and not on it).\n",
    "* Number of training points that simultaneously respect all the following conditions:\n",
    "  1. lie **strictly between the margins**\n",
    "  2. are correctly classified\n",
    "  3. are not on the decision boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(slacks):\n",
    "    '''\n",
    "    Returns the accuracy.\n",
    "    Parameters: \n",
    "        slacks (np.array, shape (N,) ): Slack variables\n",
    "    Returns:\n",
    "        accuracy (float): the ratio of correctly classified samples\n",
    "    '''\n",
    "    ### CODE HERE\n",
    "    return ...\n",
    "\n",
    "print(f'Accuracy: {accuracy(slacks)*100:.2f}%')\n",
    "\n",
    "save_student_results.accuracy(locals())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def in_correct_margin(slacks):\n",
    "    '''\n",
    "    Returns the number of samples that are classified correctly but strictly between the margins\n",
    "    Parameters: \n",
    "        slacks (np.array, shape (N,) ): Slack variables\n",
    "    Returns:\n",
    "        num_in_margin (int)\n",
    "    '''\n",
    "    ### CODE HERE\n",
    "    return ...\n",
    "print(f'Number of samples inside the margin: {in_correct_margin(slacks)} out of {slacks.shape[0]}')\n",
    "\n",
    "save_student_results.in_correct_margin(locals())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3. Weighted k-NN Classifier with Feature Expansion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train, data_test, labels_train, labels_test, class_names = helpers.load_knn_data()\n",
    "colors = np.array([[1.0, 0.0, 0], [0, 0, 1.0]])\n",
    "helpers.plot_knn_training_test(data_train, data_test, labels_train, labels_test, colors, class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"No. of examples in training set : {}\".format(data_train.shape[0]))\n",
    "print(\"No. of examples in test set     : {}\".format(data_test.shape[0]))\n",
    "print(\"No. of features in the data     : {}\".format(data_train.shape[1]))\n",
    "print(\"No. of classes present          : {}\".format(len(np.unique(labels_train))))\n",
    "\n",
    "\n",
    "plt.bar(class_names, np.bincount(labels_train))\n",
    "plt.ylabel('Count')\n",
    "plt.title('Histogram plot of each class in the training set')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Distance metrics\n",
    "\n",
    "In this exercise, we first ask you to implement both the Manhattan distance (a.k.a. L1 distance) and the cosine similarity distance. \n",
    "\n",
    "**Manhattan distance:** The Manhattan distance between two vectors $\\mathbf{v}\\in\\mathbf{R}^D$ and $\\mathbf{w}\\in\\mathbf{R}^D$ is given by\n",
    "\n",
    "$$\n",
    "\\text{Manhattan}(\\mathbf{v}, \\mathbf{w}) = { \\sum_{i=1}^{D} |\\mathbf{v}^{(i)} - \\mathbf{w}^{(i)}| }\n",
    "$$\n",
    "\n",
    "where $\\mathbf{v}^{(i)}$ denotes the value in the dimension $i$ of $\\mathbf{v}$.\n",
    "\n",
    "**Cosine similarity:** The cosine similarity is a measure of similarity between two non-zero vectors. From elementary geometry, recall that the cosine of the angle between two vectors is given by\n",
    "\n",
    "$$\n",
    "\\text{cosine}(\\mathbf{v}, \\mathbf{w}) = \\frac{ \\sum_{i=1}^{D}  \\mathbf{v}^{(i)} . \\mathbf{w}^{(i)} }{\\| \\mathbf{v} \\| \\| \\mathbf{w} \\| }\n",
    "$$\n",
    "\n",
    "Then we can define a cosine distance metric as\n",
    "\n",
    "$$\n",
    "d(\\mathbf{v}, \\mathbf{w}) = 1 - \\text{cosine}(\\mathbf{v}, \\mathbf{w})\n",
    "$$\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implement manhattan distance below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def manhattan_distance(test_example, training_examples):\n",
    "    \"\"\"\n",
    "    Computes the Manhattan distance between a single test_example \n",
    "    and the complete training data\n",
    "    \n",
    "    Parameters: \n",
    "        test_example (np.array): Data sample of shape (D,)\n",
    "        training_examples (np.array): Matrix of training data samples of shape (N,D)\n",
    "    Returns:\n",
    "        distances:  (np.array) of shape (N,)\n",
    "    \"\"\" \n",
    "    \n",
    "    #------------------ WRITE YOUR CODE HERE --------------------------\n",
    "    distances = ...\n",
    "    \n",
    "    return distances\n",
    "\n",
    "save_student_results.save_manhattan_distance(locals())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implement cosine distance below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_distance(test_example, training_examples):\n",
    "    \"\"\"\n",
    "    Computes the cosine distance between a single test example \n",
    "    and the complete training data\n",
    "    \n",
    "    Parameters: \n",
    "        test_example (np.array): Data sample of shape (D,)\n",
    "        training_examples (np.array): Matrix of training data samples of shape (N,D)\n",
    "    Returns:\n",
    "        distances :  (np.array) of shape (N,)\n",
    "    \"\"\" \n",
    "    \n",
    "    #------------------ WRITE YOUR CODE HERE --------------------------\n",
    "\n",
    "    \n",
    "    return distances\n",
    "\n",
    "save_student_results.save_cosine_distance(locals())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Weighed k-NN\n",
    "\n",
    "\n",
    "During the previous exercise session, we considered a setting in which the label of the test sample was predicted as the most frequent label in the list of nearest neighbors' labels.  \n",
    "\n",
    "Here, we ask you to implement a more sophisticated prediction scheme, where the k-nearest neighbors' labels $y_i \\in \\{0, 1\\}$ for $i \\in \\{1,..k\\}$  are weighted by the inverse of the distance $d(\\mathbf{u},\\mathbf{v_i})$ between the training sample  $\\mathbf{v}_i$ and the test sample $\\mathbf{u}$ , i.e.,\n",
    "\n",
    "$$\n",
    "y = \\text{round}\\left( \\frac{ \\sum_{i=1}^k \\frac{1}{d(\\mathbf{u},\\mathbf{v}_i)} y_i } { \\sum_{i=1}^k \\frac{1}{d(\\mathbf{u},\\mathbf{v}_i)}} \\right)\n",
    "$$\n",
    "\n",
    "\n",
    "#### Implement this weighting scheme below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_label(neighbor_labels, neighbor_distances):\n",
    "    \n",
    "    \"\"\"predict the label by weighing the nearest neighbors' label with the inverse\n",
    "    distance to the sample.\n",
    "    \n",
    "    Input:\n",
    "        neighbor_labels (np.array, int datatype): shape (k,)\n",
    "        neighbor_distances(np.array): shape (k,)\n",
    "        \n",
    "    Output:\n",
    "        output_label (integer)\n",
    "    \"\"\"\n",
    "\n",
    "    #------------------ WRITE YOUR CODE HERE --------------------------\n",
    "    \n",
    "    weighted_labels= ...\n",
    "    \n",
    "    ## np.around() returns a floating value.  (e.g. 1.0 )\n",
    "    ## we change it to integer to return the assigned label  (e.g. 1)\n",
    "    return int(np.around(weighted_labels))\n",
    "\n",
    "save_student_results.save_predict_label(locals())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Feature expansion\n",
    "\n",
    "\n",
    "In this section, we ask you to expand the original input features. \n",
    "\n",
    "Given the input data $\\mathbf{X}\\in\\mathbf{R}^{N \\times D}$, let $\\mathbf{X}^{(i)}\\in \\mathbf{R}^{N \\times 1}$ denote the values of feature $i$ for all samples, corresponding to column $i$. We expand the original features by \n",
    "\n",
    "1. computing all pairwise features $\\mathbf{X}^{(i)}  . \\mathbf{X}^{(j)}$ by multiplying any two features $\\mathbf{X}^{(i)}$ and $\\mathbf{X}^{(j)}$ for $j \\in {\\{0,...,i-1\\}}$,  $i \\in {\\{1,...,D-1\\}}$. \n",
    "  This yields $F_1= \\frac{D\\cdot(D-1)}{2}$ additional terms. \n",
    " \n",
    "<p></p>\n",
    "\n",
    "\n",
    "2. computing all terms of higher degree up to $M$ for each feature $\\mathbf{X}^{(i)}$. We compute the $m^{th}$ power of each feature $\\mathbf{X}^{(i)}$ for $m$ ranging from 2 to $M$. Because of this expansion, we get  $F_2= (M-1)\\cdot D$ additional features.\n",
    "<p></p>\n",
    "\n",
    "\n",
    "In the end, we obtain final features of dimension $F$ = $D$ + $F_1$ + $F_2$.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###### Implement the feature expansion function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Hints,\n",
    "#  You may use np.concatenate((input1, input2), axis=..) to combine the the features. \n",
    "#  Before concatenating, make sure the number of dimensions in both inputs are same \n",
    "#  Both the inputs must have the same shape, except in the dimension along which concatenation is done \n",
    "\n",
    "Input: X = [[ 1  2]\n",
    "            [ 3  4]]\n",
    "        (np.array) shape (2, 2)\n",
    "\n",
    "        M = 5\n",
    "\n",
    "Output = [[   1    2    2    1    4    1    8    1   16    1   32]\n",
    "          [   3    4   12    9   16   27   64   81  256  243 1024]]\n",
    "          \n",
    "          (np.array) shape (2,11)\n",
    "          \n",
    "The first D=2 columns are the original features. The third column contains the F1 = 1 pairwise term.\n",
    "The rest of the  F2 = 8  columns belongs to higher degree terms for m = 2, 3, 4 and 5.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def feature_expansion(X, M):\n",
    "    \"\"\"\n",
    "    Expands the given features with pairwise products and higher degree features\n",
    "    Parameters: \n",
    "        X (np.array): Data samples of shape (N,D)\n",
    "        M (integer):  expansion degree M\n",
    "    Returns:\n",
    "        expanded_X (np.array): Expanded data samples of shape (N,F), \n",
    "        where F is the total number of features after expansion as above\n",
    "        \n",
    "    \"\"\" \n",
    "    #------------------------------- WRITE YOUR CODE HERE --------------------------\n",
    "\n",
    "    \n",
    "    # ------------------------------------------------------------------------------\n",
    "        \n",
    "    return expanded_X\n",
    "\n",
    "save_student_results.save_feature_expansion(locals())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Below, we provide a helper function to find the indices of the k smallest distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the indices of the k shortest distances from a array of distances\n",
    "def find_k_nearest_neighbors(k, distances):\n",
    "    \"\"\" find indices of the k smallest distances\n",
    "    Inputs:\n",
    "        k (integer)\n",
    "        distances (np.array): shape (N,)\n",
    "    Outputs:\n",
    "        indices (np.array): shape (k,)\n",
    "    \"\"\"\n",
    "    indices = np.argsort(distances)[:k]\n",
    "    return indices        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Putting everything together (Just run all the cells below)\n",
    "Now we combine the functions we have written above to predict the label of one test example. We further provide a function **kNN** that applies **kNN_one_example** to an arbitrary number of examples (as you had in the exercise session). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kNN_one_example(unlabeled_example, training_features, training_labels, k, distance):\n",
    "    \"\"\" run kNN algorithm on one example\n",
    "    Inputs:\n",
    "        unlabeled_example (np.array): shape (D,)\n",
    "        training_features(np.array): shape (N,D)\n",
    "        training_labels(np.array): shape (N)\n",
    "        k (integer)\n",
    "        distance (string): which distance to compute\n",
    "    Outputs:\n",
    "        label (integer)\n",
    "    \"\"\"\n",
    "    \n",
    "    if distance == \"manhattan\":\n",
    "        distances = manhattan_distance(unlabeled_example, training_features)\n",
    "    elif distance == \"cosine\":\n",
    "        distances = cosine_distance(unlabeled_example, training_features)\n",
    "    \n",
    "    nn_indices = find_k_nearest_neighbors(k, distances) \n",
    "    neighbor_labels = training_labels[nn_indices]\n",
    "    neighbor_distances = distances[nn_indices]\n",
    "    best_label = predict_label(neighbor_labels, neighbor_distances) \n",
    "    return best_label\n",
    "\n",
    "def kNN(unlabeled, training_features, training_labels, k, distance):\n",
    "    return np.apply_along_axis(kNN_one_example, 1, unlabeled, training_features, training_labels, k, distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us also provide the following feature normalization function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(x, means, stds):\n",
    "    \"\"\" normalize the data using means and stds\n",
    "    Inputs:\n",
    "        x (np.array): Input data of shape (N,D)\n",
    "        means (np.array): shape (1,D)\n",
    "        stds (np.array): shape (1,D)\n",
    "        \n",
    "    Outputs:\n",
    "        normed_data: shape (N,D)\n",
    "    \"\"\"\n",
    "    \n",
    "    return (x-means)/stds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now run the cell below to see your model's results using both metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# highest degree of expansion\n",
    "M = 5\n",
    "\n",
    "data_train_expanded =  feature_expansion(data_train, M)\n",
    "data_test_expanded  =  feature_expansion(data_test, M)\n",
    "\n",
    "mean_val = data_train_expanded.mean(axis=0, keepdims=True)\n",
    "std_val  = data_train_expanded.std(axis=0, keepdims=True)\n",
    "norm_train_data_expanded = normalize(data_train_expanded, mean_val, std_val)\n",
    "norm_test_data_expanded  = normalize(data_test_expanded, mean_val, std_val)\n",
    "\n",
    "print(\"No of features in the original data: {}\".format(data_train.shape[1]))\n",
    "print(\"No of features in the expanded data: {}\\n\".format(data_train_expanded.shape[1]))\n",
    "\n",
    "# No. of nearest neighbors\n",
    "k = 5\n",
    "\n",
    "# run weighted-KNN classifier with both distance metrics\n",
    "for distance_type in [\"manhattan\",  \"cosine\"]:\n",
    "    predicted_labels_test = kNN(norm_test_data_expanded, norm_train_data_expanded, \n",
    "                                labels_train, k, distance_type)\n",
    "    \n",
    "    accuracy = helpers.my_accuracy_func(predicted_labels_test, labels_test)\n",
    "    print(\"Distance type: {:12s} Test Accuracy: {:.2f}%\".format(distance_type, 100*accuracy))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

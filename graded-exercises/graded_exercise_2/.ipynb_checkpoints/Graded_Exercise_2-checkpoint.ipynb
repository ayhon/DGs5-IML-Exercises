{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "mpl.rc('figure', max_open_warning = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import helpers.pytorch_helpers as pytorch_helpers\n",
    "import grading.save_student_results as save_student_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graded Exercise Session 2 \n",
    "\n",
    "**Date: 10.12.2021**\n",
    "\n",
    "Welcome to the second graded exercise session. \n",
    "\n",
    "You are asked to fill in the code in a few cells throughout the exercise. At the end of each cell where we ask you to fill in some code, you will notice a call to a function from the `save_student_results` module. This ensures that the body of your function is run with pre-generated data and your current results are saved to a file (which you will eventually submit to Moodle). The cells are independent of each other and you will receive points for each individual cell. We will not grant partial points within a cell.\n",
    "\n",
    "Before you finish, please make sure to **upload two files to Moodle**:\n",
    "* **graded_exercise_2.ipynb**\n",
    "* **answers_SCIPER.npz (e.g. \"answers_280595.npz\")**\n",
    "\n",
    "Good luck! :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sciper_number = 342296  # e.g. 123456\n",
    "save_student_results.initialize_res(locals())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1. MLP: \n",
    "\n",
    "In this part, we will be testing an already trained two-layered MLP. \n",
    "\n",
    "First let's warm up by coding the `tanh` activation function. This activation function is defined as:\n",
    "\n",
    "\\begin{align}\n",
    "\\text{tanh}(z) = \\frac{e^{z}-e^{-z}}{e^{z}+e^{-z}}\n",
    "\\end{align}\n",
    "\n",
    "Its gradient is defined as:\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{d\\text{tanh}(z)}{dz} = 1-(\\text{tanh}(z))^2\n",
    "\\end{align}\n",
    "\n",
    "Now, let's code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tanh:\n",
    "    @staticmethod\n",
    "    def forward(z):\n",
    "        ### YOUR CODE HERE\n",
    "        return (np.exp(z) - np.exp(-z))/(np.exp(z) + np.exp(-z))\n",
    "        \n",
    "    @staticmethod\n",
    "    def gradient(z):\n",
    "        ### YOUR CODE HERE\n",
    "        return 1 - Tanh.forward(z)**2\n",
    "    \n",
    "save_student_results.save_tanh(locals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let's test whether our functions work\n",
    "z = np.linspace(-10,10,1000)\n",
    "T = Tanh.forward(z)\n",
    "dT_dz = Tanh.gradient(z)\n",
    "\n",
    "plt.plot(z, T)\n",
    "plt.title('Tanh')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(z, dT_dz)\n",
    "plt.title('Derivative Tanh')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's define the following neural network: \n",
    "* The first layer (input layer): Our network will take an input of 5 dimensions.\n",
    "* Hidden layer 1: 10 nodes.\n",
    "* Hidden layer 2: 12 nodes. \n",
    "* Final layer (output layer): Our network returns an output of 3 dimensions. \n",
    "\n",
    "Below, we provide an image depicting this network. (If there is no image showing in your notebook, check the helpers folder for mlp.png)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![MLP](helpers/mlp.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass (X, weights1, bias1, weights2, bias2, weights3, bias3, activation=Tanh):\n",
    "    \"\"\"\n",
    "        Forward pass of our MLP, using current weights and biases. Returns the output of the network.\n",
    "        Parameters:\n",
    "            X (np.array): Input, of shape (N,D_input), where D_input=5. \n",
    "            weights1 (np.array): Weights between the input layer and hidden layer 1, of shape (5,10)\n",
    "            bias1(np.array): Bias of hidden layer 1 of shape (10,)\n",
    "            weights2 (np.array): Weights between hidden layer 1 and hidden layer 2, of shape (10,12)\n",
    "            bias2(np.array): Bias of hidden layer 2 of shape (12,)\n",
    "            weights3 (np.array): Weights between hidden layer 2 and the output layer, of shape (12,3)\n",
    "            bias3(np.array): Bias of layer 3, of shape (3,)\n",
    "\n",
    "        :return:\n",
    "            output (np.array): Output, of shape (N,D_output), where D_output=3\n",
    "    \"\"\"\n",
    "    ## YOUR CODE HERE\n",
    "    ...\n",
    "    return ...\n",
    "\n",
    "save_student_results.save_forward_pass(locals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## A quick test\n",
    "\n",
    "X = np.random.normal(0,1,(100,5))\n",
    "weights1 = np.random.normal(0,1,(5,10))\n",
    "bias1 = np.random.normal(0,1,(10,))\n",
    "weights2 = np.random.normal(0,1,(10,12))\n",
    "bias2 = np.random.normal(0,1,(12,))\n",
    "weights3 = np.random.normal(0,1,(12,3))\n",
    "bias3 = np.random.normal(0,1,(3,))\n",
    "\n",
    "out = forward_pass(X, weights1, bias1, weights2, bias2, weights3, bias3, Tanh)\n",
    "print(\"This shape should be (100,3):\", out.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2. Optimization Using PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As discussed during a previous exercise session, PyTorch allows for automatic differentiation of standard mathematical operations. In other words, gradients of standard operations are computed automatically. Gradient descent can thus be performed very easily, without having to manually define or compute gradients.\n",
    "\n",
    "In this exercise, we exploit this to minimize a toy 1D function $f$ that is differentiable:\n",
    "\\begin{align*}\n",
    "f: & \\mathbb{R} \\rightarrow \\mathbb{R} \\\\ \n",
    " & x \\mapsto f(x)\n",
    "\\end{align*}\n",
    "\n",
    "The goal is to find $x^*$ such that $f(x^*)$ is a local minimum. Such an $x^*$ can be numerically estimated by using gradient descent. Starting from $x_0 \\in \\mathbb{R}$, we iteratively update it for $k \\leq K$ steps using\n",
    "$$\n",
    "x_{k+1} = x_k - \\eta \\cdot f'(x_k)\n",
    "$$\n",
    "where the scalar $\\eta$ is the learning rate.\n",
    "\n",
    "PyTorch automates for us both the computation of the derivative $f'(x_k)$, and the gradient descent step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Implement a function in pytorch\n",
    "\n",
    "The goal is to code the following formula: \n",
    "\n",
    "$$\n",
    "f(x) = \\frac{\\sin(3.14 x)}{|x|+1} + 2\\exp\\big({-\\frac{(x-4.5)^2}{2}}\\big)\n",
    "$$\n",
    "\n",
    "For computing $\\sin(\\cdot)$ and $\\exp(\\cdot)$, you must **use PyTorch functions** ```torch.sin``` and ```torch.exp``` instead of the numpy ones (```np.sin``` and ```np.exp```). Using PyTorch versions of these functions allows it to compute gradients, which numpy does not do.\n",
    "\n",
    "If the formula is implemented correctly, you must get the following plot:\n",
    "\n",
    "![func](helpers/func.png \"Function to code\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    ### YOUR CODE HERE\n",
    "    ...\n",
    "    ###\n",
    "    \n",
    "    return ...\n",
    "    ###\n",
    "\n",
    "## Save results for grading\n",
    "save_student_results.f(locals())\n",
    "    \n",
    "# And plot it\n",
    "pytorch_helpers.plot_function(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Initialize the starting point and the optimizer\n",
    "\n",
    "We need to define one PyTorch tensor storing the value of $x_0$, as well as the optimizer for doing gradient descent steps. \n",
    "The goal is to code an `init_x_and_optim` function that does both. This function returns:\n",
    "\n",
    "* One initial point:\n",
    "    - The `float` value of $x_0$ that must be converted to a `torch.tensor` (equivalent to a numpy array, but in pytorch).\n",
    "    - By default, all newly created tensors in PyTorch do not require gradients computation. For the gradient to be computed, you need to manually set the `requires_grad` attribute to True.\n",
    "* One optimizer, handling the gradient descent steps for us:\n",
    "    - In this exercise, we use a predefined Stochastic Gradient Descent (SGD) optimizer from PyTorch, created using ```torch.optim.SGD(...)```. As the parameters to optimize, we pass the values of $x_0$\n",
    "    \n",
    "    \n",
    "More generally, PyTorch [documentation and examples](https://pytorch.org/docs/stable/optim.html) on how to perform one gradient descent can be very useful (for this part, you are allowed to look at this webpage)! Also, don't hesitate to look at the previous exercise sessions which also used PyTorch.\n",
    "\n",
    "We have provided the implementation for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_x_and_optim(x_0_value, lr):\n",
    "    \"\"\"\n",
    "    Define one initial value and associated optimizer with a given learning rate.\n",
    "    \n",
    "    Inputs:\n",
    "        x_0_value : float, the initial value of x_0\n",
    "        lr : float, optimizer's learning rate\n",
    "    Outputs:\n",
    "        x_0 : torch tensor of shape (1), whose single value is x_0_value\n",
    "                x_0 must have attribute .requires_grad = True\n",
    "        opt : torch SGD optimizer on the x_0 tensor, with learning rate lr\n",
    "    \"\"\"\n",
    "    # Create a pytorch tensor (equivalent to numpy array)\n",
    "    # of one single element, whose value is x_0_value\n",
    "    x_0 = torch.tensor([x_0_value])\n",
    "    \n",
    "    # If you have not already done it while initializing x_0 above,\n",
    "    # tell pytorch we are interested in getting the gradients of this tensor\n",
    "    x_0.requires_grad = True\n",
    "    \n",
    "    # Initialize one optimizer: SGD, whose goal is to optimize x_0, using learning rate lr\n",
    "    opt = torch.optim.SGD([x_0], lr=lr)\n",
    "    \n",
    "    return x_0, opt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Minimize the function\n",
    "\n",
    "The goal of this part is to write a routine to minimize the function $f$ via automatic gradient computation in PyTorch. If the function is implemented correctly, you must get the following descent plot (from $x_0$ in blue to $x_{final}$ in red):\n",
    "\n",
    "![func](helpers/descent.png \"Function to code\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_0_VALUE = 3. # initial point of x\n",
    "LR = 0.01 # learning rate\n",
    "NUM_STEPS = 1000 # number of gradient steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient descent loop encapsulated in a function\n",
    "def minimize(f, x_0, opt, n_steps):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        f : function R -> R whose output we want to minimize using gradient descent\n",
    "        x_0 : torch tensor of shape (1), whose single value is the initial guess for minimizing f\n",
    "                x_0 must have attribute .requires_grad = True\n",
    "        opt : torch SGD optimizer on the x_0 tensor\n",
    "        n_steps : int, the number of gradient descent steps to perform\n",
    "    Outputs:\n",
    "        trace : list containing n_steps pairs representing the successive\n",
    "                inputs and outputs to f during the descent:\n",
    "                    [(x_0, f(x_0)), (x_1, f(x_1)), ...., (x_{n_steps}, f(x_{n_steps}))]\n",
    "                WARNING: Each tuple is a tuple of python floats, not torch tensors\n",
    "                From a torch tensor x containing a single value, the corresponding value\n",
    "                can be obtained via x.item()     \n",
    "    \"\"\"\n",
    "    # For accumulating points during the descent\n",
    "    trace = []\n",
    "    \n",
    "    # Iterate for n_steps:\n",
    "    for k in range(n_steps):\n",
    "        # Initialize the gradients to zero\n",
    "        ### YOUR CODE HERE\n",
    "        ...\n",
    "        ###\n",
    "\n",
    "        # Compute the function's output\n",
    "        ### YOUR CODE HERE\n",
    "        ...\n",
    "        ###\n",
    "\n",
    "        \n",
    "        # Ask pytorch to compute the gradient: we want to minimize y\n",
    "        ### YOUR CODE HERE\n",
    "        ...\n",
    "        ###\n",
    "\n",
    "\n",
    "        # Do one step of gradient descent with opt\n",
    "        ### YOUR CODE HERE\n",
    "        ...\n",
    "        ###\n",
    "\n",
    "        \n",
    "        # Accumulate the current (x,f(x)) point\n",
    "        trace.append((x_0.item(), y.item()))\n",
    "    \n",
    "    return trace\n",
    "\n",
    "\n",
    "## Save results for grading\n",
    "save_student_results.minimize(locals())\n",
    "\n",
    "\n",
    "# Initialize the optimization\n",
    "x_0, optimizer = init_x_and_optim(X_0_VALUE, LR)\n",
    "# Do the actual minimization\n",
    "optimization_trace = minimize(f, x_0, optimizer, NUM_STEPS)\n",
    "# We plot the descent on the graph of f\n",
    "# (only for 1/10th of the trace points)\n",
    "pytorch_helpers.plot_descent(f, optimization_trace[::10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Maximize the function\n",
    "\n",
    "PyTorch can very easily be used to perform gradient **ascent**, i.e., to maximize a function. With code very similar to what you wrote in the first part of the exercise for minimizing function $f$, you are now asked to maximize it.\n",
    "\n",
    "The initialization step (creating an initial tensor and optimizer) is the same and does not need to be recoded. However, ```minimize()``` was coded to perform gradient descent. Now, with only a slight variation, you are asked to code ```maximize()```, to perform gradient ascent.\n",
    "\n",
    "*Hint*: maximizing a function $f$ is equivalent to minimizing $-f$. \n",
    "\n",
    "If the function is implemented correctly, you must get the following ascent plot (from $x_0$ in blue to $x_{final}$ in red):\n",
    "\n",
    "![func](helpers/ascent.png \"Function to code\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient ASCENT loop encapsulated in a function\n",
    "def maximize(f, x_0, opt, n_steps):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        f : function R -> R whose output we want to minimize using gradient ascent\n",
    "        x_0 : torch tensor of shape (1), whose single value is the initial guess for maximizing g\n",
    "                x_0 must have attribute .requires_grad = True\n",
    "        opt : torch SGD optimizer on the x_0 tensor\n",
    "        n_steps : int, the number of gradient ascent steps to perform\n",
    "    Outputs:\n",
    "        trace : list containing n_steps 2-tuples representing the successive\n",
    "                inputs and outputs to f during the ascent:\n",
    "                    [(x_0, f(x_0)), (x_1, f(x_1)), ...., (x_{n_steps}, f(x_{n_steps}))]\n",
    "                WARNING: Each tuple is a tuple of python floats, not torch tensors\n",
    "                From a torch tensor x containing a single value, the corresponding value\n",
    "                can be obtained via x.item()     \n",
    "    \"\"\"\n",
    "    # For accumulating points during the ascent\n",
    "    trace = []\n",
    "    \n",
    "    # Iterate for n_steps:\n",
    "    for k in range(n_steps):\n",
    "        # Initialize the gradients\n",
    "        ### YOUR CODE HERE\n",
    "        ...\n",
    "        ###\n",
    "\n",
    "        \n",
    "        # Compute the function's output\n",
    "        ### YOUR CODE HERE\n",
    "        ...\n",
    "        ###\n",
    "    \n",
    "        \n",
    "        # Compute the gradient: we want to MAXIMIZE y, ie. MINIMIZE (-y)\n",
    "        ### YOUR CODE HERE\n",
    "        ...\n",
    "        ###\n",
    "\n",
    "        \n",
    "        # Do one step of gradient descent\n",
    "        ### YOUR CODE HERE\n",
    "        ...\n",
    "        ###\n",
    "\n",
    "        \n",
    "        # Accumulate the current (x,g(x)) point\n",
    "        trace.append((x_0.item(), y.item()))\n",
    "    return trace\n",
    "\n",
    "\n",
    "## Save results for grading\n",
    "save_student_results.maximize(locals())\n",
    "\n",
    "\n",
    "# Initialize the optimization\n",
    "x_0, optimizer = init_x_and_optim(X_0_VALUE, LR)\n",
    "# Do the actual maximization\n",
    "optimization_trace = maximize(f, x_0, optimizer, NUM_STEPS)\n",
    "# We plot the ascent on the graph of f\n",
    "# (only for 1/10th of the trace points)\n",
    "pytorch_helpers.plot_descent(f, optimization_trace[::10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3. Bag of Words Representation\n",
    "\n",
    "### 1. Load the dataset\n",
    "\n",
    "In this exercise, we ask you to represent texts as vectors using Bag of Words (BoW). Our dataset $R=\\{r_i\\}_{i=0}^{N-1}$ consists of $N$ movie reviews $r_i$. Each review ${r_i}$ is encoded as a list of words. Our aim is to construct an $N \\times K$ BoW representation array $X$, where $K$ is the number of unique words in the whole dataset.\n",
    "\n",
    "First let us begin by running the following cells to load the dataset. **You do not need to implement anything in this first section.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is an example review:\n",
      " one of the other reviewers has mentioned that after watching just 1 oz episode youll be hooked they are right as this is exactly what happened with me the first thing that struck me about oz was its brutality and unflinching scenes of violence which set in right from the word go trust me this is not a show for the faint hearted or timid this show pulls no punches with regards to drugs sex or violence its is hardcore in the classic use of the word it is called oz as that is the nickname given to the oswald maximum security state penitentary it focuses mainly on emerald city an experimental section of the prison where all the cells have glass fronts and face inwards so privacy is not high on the agenda em city is home to manyaryans muslims gangstas latinos christians italians irish and moreso scuffles death stares dodgy dealings and shady agreements are never far away i would say the main appeal of the show is due to the fact that it goes where other shows wouldnt dare forget pretty pictures painted for mainstream audiences forget charm forget romanceoz does not mess around the first episode i ever saw struck me as so nasty it was surreal i could not say i was ready for it but as i watched more i developed a taste for oz and got accustomed to the high levels of graphic violence not just violence but injustice crooked guards wholl be sold out for a nickel inmates wholl kill on order and get away with it well mannered middle class inmates being turned into prison bitches due to their lack of street skills or prison experience watching oz you may become comfortable with what is uncomfortable viewingthats if you can get in touch with your darker side\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import string\n",
    "\n",
    "# Read the dataset and clean it\n",
    "with open('helpers/IMDB_Dataset_subset_0_cleaned.csv') as file:\n",
    "    csvreader = csv.reader(file)\n",
    "    header = next(csvreader)\n",
    "    reviews_org = []\n",
    "    for review, sentiment in csvreader:\n",
    "        reviews_org.append(review)\n",
    "\n",
    "# Index for the example review\n",
    "idx = 0\n",
    "print(\"Here is an example review:\\n {}\".format(reviews_org[idx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We split each review as list of words and  exclude the words that contains digits \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "And this is the same example after we preprocess it:\n",
      " ['one', 'of', 'the', 'other', 'reviewers', 'has', 'mentioned', 'that', 'after', 'watching', 'just', 'oz', 'episode', 'youll', 'be', 'hooked', 'they', 'are', 'right', 'as', 'this', 'is', 'exactly', 'what', 'happened', 'with', 'me', 'the', 'first', 'thing', 'that', 'struck', 'me', 'about', 'oz', 'was', 'its', 'brutality', 'and', 'unflinching', 'scenes', 'of', 'violence', 'which', 'set', 'in', 'right', 'from', 'the', 'word', 'go', 'trust', 'me', 'this', 'is', 'not', 'a', 'show', 'for', 'the', 'faint', 'hearted', 'or', 'timid', 'this', 'show', 'pulls', 'no', 'punches', 'with', 'regards', 'to', 'drugs', 'sex', 'or', 'violence', 'its', 'is', 'hardcore', 'in', 'the', 'classic', 'use', 'of', 'the', 'word', 'it', 'is', 'called', 'oz', 'as', 'that', 'is', 'the', 'nickname', 'given', 'to', 'the', 'oswald', 'maximum', 'security', 'state', 'penitentary', 'it', 'focuses', 'mainly', 'on', 'emerald', 'city', 'an', 'experimental', 'section', 'of', 'the', 'prison', 'where', 'all', 'the', 'cells', 'have', 'glass', 'fronts', 'and', 'face', 'inwards', 'so', 'privacy', 'is', 'not', 'high', 'on', 'the', 'agenda', 'em', 'city', 'is', 'home', 'to', 'manyaryans', 'muslims', 'gangstas', 'latinos', 'christians', 'italians', 'irish', 'and', 'moreso', 'scuffles', 'death', 'stares', 'dodgy', 'dealings', 'and', 'shady', 'agreements', 'are', 'never', 'far', 'away', 'i', 'would', 'say', 'the', 'main', 'appeal', 'of', 'the', 'show', 'is', 'due', 'to', 'the', 'fact', 'that', 'it', 'goes', 'where', 'other', 'shows', 'wouldnt', 'dare', 'forget', 'pretty', 'pictures', 'painted', 'for', 'mainstream', 'audiences', 'forget', 'charm', 'forget', 'romanceoz', 'does', 'not', 'mess', 'around', 'the', 'first', 'episode', 'i', 'ever', 'saw', 'struck', 'me', 'as', 'so', 'nasty', 'it', 'was', 'surreal', 'i', 'could', 'not', 'say', 'i', 'was', 'ready', 'for', 'it', 'but', 'as', 'i', 'watched', 'more', 'i', 'developed', 'a', 'taste', 'for', 'oz', 'and', 'got', 'accustomed', 'to', 'the', 'high', 'levels', 'of', 'graphic', 'violence', 'not', 'just', 'violence', 'but', 'injustice', 'crooked', 'guards', 'wholl', 'be', 'sold', 'out', 'for', 'a', 'nickel', 'inmates', 'wholl', 'kill', 'on', 'order', 'and', 'get', 'away', 'with', 'it', 'well', 'mannered', 'middle', 'class', 'inmates', 'being', 'turned', 'into', 'prison', 'bitches', 'due', 'to', 'their', 'lack', 'of', 'street', 'skills', 'or', 'prison', 'experience', 'watching', 'oz', 'you', 'may', 'become', 'comfortable', 'with', 'what', 'is', 'uncomfortable', 'viewingthats', 'if', 'you', 'can', 'get', 'in', 'touch', 'with', 'your', 'darker', 'side']\n",
      "Number of words in the example review : 305\n",
      "\n",
      "The number of samples (i.e., reviews) in our dataset is: 149\n"
     ]
    }
   ],
   "source": [
    "def containsNumber(value):\n",
    "    for character in value:\n",
    "        if character.isdigit():\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "reviews_no_digits = []\n",
    "for review in reviews_org:\n",
    "    review_split = [ r for r in review.split() if not containsNumber(r)]\n",
    "    reviews_no_digits.append(review_split)\n",
    "\n",
    "reviews = reviews_no_digits\n",
    "\n",
    "idx = 0\n",
    "print(\"And this is the same example after we preprocess it:\\n {}\".format(reviews[idx]))\n",
    "print(\"Number of words in the example review : {}\\n\".format(len(reviews[idx])))\n",
    "print(\"The number of samples (i.e., reviews) in our dataset is:\", len(reviews))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 2. Bag of Words for each review\n",
    "\n",
    "In this part, you are given a ready function that represents each review as BoWs. The function generates a list $B = \\{b_i\\}_{i=0}^{N-1}$. The elements of $B$ are entries of the form $b_i = \\{\\tilde{w}_{i}^{(j)} : m_{i}^{(j)}\\}_{j=0}^{\\tilde{C}_i-1}$, where $\\tilde{C}_i$ is the number of unique words in $r_i$, $\\tilde{w}_{i}^{(j)}$ is the j-th unique word in review $r_i$ and $m_{i}^{(j)}$ is the number of times the word $\\tilde{w}_{i}^{(j)}$ occurs in review $r_i$.\n",
    "\n",
    "For example, for the following raw inputs:\n",
    "\n",
    "[\"Machine machine learning learning machine. Is is is. Really fun really fun fun fun fun.\",\n",
    "\"Machine learning. is is. Awesome awesome\"]\n",
    "\n",
    "You have the pre-processed inputs:\n",
    "\n",
    "[['machine', 'machine', 'learning', 'learning', 'machine', 'is', 'is', 'is', 'really', 'fun', 'really', 'fun', 'fun', 'fun', 'fun'],\n",
    "['machine', 'learning', 'is', 'is', 'awesome', 'awesome']]\n",
    "\n",
    "The resulting list should look like:\n",
    "\n",
    "[{'machine': 3, 'learning': 2, 'is': 3, 'really':2, 'fun':5}, {'machine': 1, 'learning': 1, 'is': 2, 'awesome': 2}]\n",
    "\n",
    "**You do not need to implement anything in this section either.** You need to use the function below in the following parts. Let's see the output of the function.\n",
    "\n",
    "#### Implementation is given below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_bow(reviews):\n",
    "    \"\"\"\n",
    "    Represents each review as Bag of Words (BoW). \n",
    "    Creates a list of (key,value) pairs, where a key is a unique word in the input sentence and the corresponding value is the number of occurences of this word\n",
    "    \n",
    "    Parameters: \n",
    "        reviews (List): List of sentences, where each sentence is also a list of words.\n",
    "    Returns:\n",
    "        bow_list (List): List of dictionaries, where each dictionary is a Bag of Words representing the corresponding sentence.\n",
    "    \n",
    "    \"\"\"\n",
    "    bow_list = []\n",
    "    for review in reviews:\n",
    "        bow_review = {}\n",
    "        for word in review:\n",
    "            if word not in bow_review.keys():\n",
    "                bow_review[word] = 1\n",
    "            else:\n",
    "                bow_review[word] += 1\n",
    "        bow_list.append(bow_review)\n",
    "    return bow_list\n",
    "\n",
    "dummy_reviews = [['machine', 'machine', 'learning', 'learning', 'machine', 'is', 'is', 'is', 'really', 'fun', 'really', 'fun', 'fun', 'fun', 'fun'],\n",
    "['machine', 'learning', 'is', 'is', 'awesome', 'awesome']]\n",
    "dummy_bow_list = generate_bow(dummy_reviews)\n",
    "print(dummy_bow_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's test this:\n",
    "bow_list = generate_bow(reviews)\n",
    "print(\"Here is the Bag of Words representation of the example review we picked in the beginning:\\n {}\".format(bow_list[idx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 3 Creating a common dictionary of unique words for the whole dataset\n",
    "\n",
    "In this part, given the BoW representations $B$ from the previous part and a list of stopwords $S$, you are asked to create a common dictionary $D$ that includes the unique words used in the whole dataset (without repetition), discarding the stopwords in $S$. The dictionary $D$ should map each word $\\hat{w}_k$ to an unique integer index $k$:  $D = \\{\\hat{w}_k : k\\}_{k=0}^{K-1}$. Note that the keys in dictionary $D$ can be arranged in any order. In other words, each word in dictionary is mapped into an unique integer k and order of words is not important.\n",
    "\n",
    "If our dataset was consisting of the two sentences provided in the beginning, the input BoW representation would look like:\n",
    "\n",
    "[{'machine': 3, 'learning': 2, 'is': 3, 'really': 2, 'fun': 5}, {'machine': 1, 'learning': 1, 'is': 2, 'awesome': 2}]\n",
    "\n",
    "We can take the stopwords as:\n",
    "\n",
    "['am', 'is', 'are']\n",
    "\n",
    "The output can look like:\n",
    "\n",
    "{'machine': 0, 'learning': 1, 'really': 2, 'fun': 3, 'awesome': 4 }\n",
    "\n",
    "Since the keys in dictionary can be stored in any order, for example, the output can also stored as:\n",
    "{'learning': 0, 'machine': 1, 'fun': 2, 'really': 3,   'awesome': 4} and in many other ways such that index associated with each word is unique.\n",
    "\n",
    "\n",
    "**This function will be implemented by you.**\n",
    "\n",
    "#### Implement the function below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_dictionary(bow_list, stopwords):\n",
    "    \"\"\"\n",
    "    Constructs a common dictionary consisting of the unique words found in the bow_list and omits the stop words. Keys of the dictionary are words and values are integer indices starting from 0 and increasing for each new entry in the dictionary.\n",
    "    \n",
    "    Parameters: \n",
    "        bow_list (List): List of BoW representations for N sentences.\n",
    "        stopwords (List): List of words to be omitted when constructing the dictionary\n",
    "    Returns:\n",
    "        dictionary (Dict): A dictionary of unique words, where the keys are the words and the values are integer indices of the words\n",
    "    \"\"\" \n",
    "    \n",
    "    ##YOUR CODE HERE\n",
    "    dictionary = {}\n",
    "    ...\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    return dictionary\n",
    "\n",
    "## Save results for grading\n",
    "save_student_results.construct_dictionary(locals())\n",
    "\n",
    "\n",
    "dummy_stopwords = ['am','is','are']\n",
    "dummy_dictionary = construct_dictionary(dummy_bow_list, dummy_stopwords)\n",
    "print(dummy_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = ['', 'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves',\n",
    "                 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
    "                 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself',\n",
    "                 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves',\n",
    "                 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those',\n",
    "                 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having',\n",
    "                 'do', 'does', 'did', 'doing', 'a', 'an', 'the',\n",
    "                 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while',\n",
    "                 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through',\n",
    "                 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down',\n",
    "                 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once',\n",
    "                 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each',\n",
    "                 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only',\n",
    "                 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just',\n",
    "                 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'could', 'would',\n",
    "                 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\",\n",
    "                 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'might',\n",
    "                 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\",\n",
    "                 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
    "\n",
    "dictionary = construct_dictionary(bow_list, stopwords)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Vector representation of BoW\n",
    "\n",
    "In this part, you are asked to encode each review as a vector representation of the corresponding BoW. \n",
    "Given the dictionary $D$ and the BoW representation $B$, we first define $\\tilde{X} \\in \\mathbb{R}^{N \\times K}$ and let its rows $\\tilde{x}_i \\in \\mathbb{R}^{1 \\times K}$ be the unnormalized BoW representation vector corresponding to $b_i$. That is, $\\tilde{x}_i^{(k)}$ is the number of times word $\\hat{w}_k$ from the dictionary appears in review $r_i$. Hence,\n",
    "\n",
    "\n",
    "$$\n",
    "\\\n",
    "    \\tilde{x}_i^{(k)} =\n",
    "\\begin{cases}\n",
    "    b_i[\\hat{w}_k],& \\text{if } \\hat{w}_k \\text{is in the keys of $b_i$} \\\\\n",
    "    0,          & \\text{otherwise}\n",
    "\\end{cases}\n",
    "\\\n",
    "$$\n",
    "\n",
    "Note that $b_i$ are dictionaries that relate words to number of occurences. When a word $w$ is used in review $r_i$, its BoW representation $b_i$ stores the number of occurences of that word $w$ as $b[w]$.\n",
    "\n",
    "<!-- $$\n",
    "b_i[w] = m_{i}^{(v)}, \\text{if } \\exists v \\in \\{0,...,\\tilde{C}_i-1\\} \\text{ such that } w = \\tilde{w}_i^{(v)} \\in \\{\\tilde{w}_i^{(j)}\\}_{j=0}^{\\tilde{C}_i-1}\n",
    "$$\n",
    " -->\n",
    "Our aim then is to construct the normalized BoW representation array $X \\in \\mathbb{R}^{N \\times K}$, whose rows are $x_i \\in \\mathbb{R}^{1 \\times K}$, such that\n",
    "\n",
    "$$\n",
    "x_i^{(j)} = \\frac{\\tilde{x}_i^{(j)}} {\\sum_{\\ell=1}^{K} \\tilde{x}_i^{(\\ell)}}\n",
    "$$\n",
    "\n",
    "If we continue with the same example as before, using our dictionary:\n",
    "\n",
    "{'machine': 0, 'learning': 1, 'really':2, 'fun':3, 'awesome':4 }\n",
    "\n",
    "and the BoWs:\n",
    "\n",
    "[{'machine': 3, 'learning': 2, 'is': 3, 'really': 2, 'fun': 5}, {'machine': 1, 'learning': 1, 'is': 2, 'awesome': 2}]\n",
    "\n",
    "we obtain unnormalized BoWs vectors and normalized BoW vectors:\n",
    "\n",
    "[[3, 2, 2, 5, 0], [1, 1, 0, 0, 2]] -> [[3/12, 2/12, 2/12, 5/12, 0/12], [1/4, 1/4, 0/4, 0/4, 2/4]]\n",
    "\n",
    "Note that you should use the indices stored in values of $D$ to create the vector representations\n",
    "\n",
    "#### Implement the function below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bow_array(bow_list, dictionary):\n",
    "    \"\"\"\n",
    "    Encodes each sentence as a vector representation of the input Bag of Words.\n",
    "    \n",
    "    Parameters: \n",
    "        bow_list (List): List of N BoW dictionaries.\n",
    "        dictionary (Dict): A dictionary of K unique words, where keys are the words and values are integer indices in {0,...,K-1}\n",
    "    Returns:\n",
    "        X (np.array): Normalized BoW representation array of shape (N,K)\n",
    "    \"\"\" \n",
    "    \n",
    "    n_sentences = len(bow_list)\n",
    "    n_dictionary = len(dictionary)\n",
    "    X_tilde = np.zeros((n_sentences, n_dictionary))\n",
    "    # WRITE YOUR CODE HERE\n",
    "    ...\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    return X\n",
    "\n",
    "## Save results for grading\n",
    "save_student_results.get_bow_array(locals())\n",
    "\n",
    "dummy_bow_array = get_bow_array(dummy_bow_list, dummy_dictionary)\n",
    "print(dummy_bow_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_array = get_bow_array(bow_list, dictionary)\n",
    "\n",
    "print(bow_array)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

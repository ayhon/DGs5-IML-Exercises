{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Introduction\n",
    "\n",
    "In this week's exercise session we will learn how to use Machine Learning methods to solve regression problems. In particular, we will focus on linear regression. \n",
    "\n",
    "First let us import the libraries we will be using during this exercise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 The regression problem\n",
    "\n",
    "We have seen in class that regression refers to predicting continous values for a given sample. A nice example could be predicting the first salary of a student after graduation given how diligent they were with attending machine learning exercise sessions. We were introduced to the \"linear regression method\". \n",
    "\n",
    "**Q: How does a regression problem differ from a classification problem?**\n",
    "\n",
    "**Q: Why is the linear regression model a linear model?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1 Load and inspect the data\n",
    "\n",
    "This week, using the linear regression method, we will analyze the Boston house prices data set and predict costs based on properties such as per capita crime rate by town, pupil-teacher ratio by town, etc.\n",
    "\n",
    "We load the data and split it such that 80% and 20% are training and test data, respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _boston_dataset:\n",
      "\n",
      "Boston house prices dataset\n",
      "---------------------------\n",
      "\n",
      "**Data Set Characteristics:**  \n",
      "\n",
      "    :Number of Instances: 506 \n",
      "\n",
      "    :Number of Attributes: 13 numeric/categorical predictive. Median Value (attribute 14) is usually the target.\n",
      "\n",
      "    :Attribute Information (in order):\n",
      "        - CRIM     per capita crime rate by town\n",
      "        - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\n",
      "        - INDUS    proportion of non-retail business acres per town\n",
      "        - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
      "        - NOX      nitric oxides concentration (parts per 10 million)\n",
      "        - RM       average number of rooms per dwelling\n",
      "        - AGE      proportion of owner-occupied units built prior to 1940\n",
      "        - DIS      weighted distances to five Boston employment centres\n",
      "        - RAD      index of accessibility to radial highways\n",
      "        - TAX      full-value property-tax rate per $10,000\n",
      "        - PTRATIO  pupil-teacher ratio by town\n",
      "        - B        1000(Bk - 0.63)^2 where Bk is the proportion of black people by town\n",
      "        - LSTAT    % lower status of the population\n",
      "        - MEDV     Median value of owner-occupied homes in $1000's\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "\n",
      "    :Creator: Harrison, D. and Rubinfeld, D.L.\n",
      "\n",
      "This is a copy of UCI ML housing dataset.\n",
      "https://archive.ics.uci.edu/ml/machine-learning-databases/housing/\n",
      "\n",
      "\n",
      "This dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.\n",
      "\n",
      "The Boston house-price data of Harrison, D. and Rubinfeld, D.L. 'Hedonic\n",
      "prices and the demand for clean air', J. Environ. Economics & Management,\n",
      "vol.5, 81-102, 1978.   Used in Belsley, Kuh & Welsch, 'Regression diagnostics\n",
      "...', Wiley, 1980.   N.B. Various transformations are used in the table on\n",
      "pages 244-261 of the latter.\n",
      "\n",
      "The Boston house-price data has been used in many machine learning papers that address regression\n",
      "problems.   \n",
      "     \n",
      ".. topic:: References\n",
      "\n",
      "   - Belsley, Kuh & Welsch, 'Regression diagnostics: Identifying Influential Data and Sources of Collinearity', Wiley, 1980. 244-261.\n",
      "   - Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get the data set and print a description\n",
    "from sklearn.datasets import load_boston\n",
    "boston_dataset = load_boston()\n",
    "print(boston_dataset.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the data sample matrix X: (475, 12)\n",
      "The number of data samples is N: 475\n",
      "The number of features is D: 12\n",
      "\n",
      "Shape of the labels vector y: (475,)\n",
      "\n",
      "First data sample in X: [6.320e-03 1.800e+01 2.310e+00 5.380e-01 6.575e+00 6.520e+01 4.090e+00\n",
      " 1.000e+00 2.960e+02 1.530e+01 3.969e+02 4.980e+00]\n",
      "First label in y: 24.0\n"
     ]
    }
   ],
   "source": [
    "X = boston_dataset[\"data\"]\n",
    "y = boston_dataset[\"target\"]\n",
    "\n",
    "# remove categorical feature\n",
    "X = np.delete(X, 3, axis=1)\n",
    "# removing second mode\n",
    "ind = y<40\n",
    "X = X[ind,:]\n",
    "y = y[ind]\n",
    "\n",
    "print(\"Shape of the data sample matrix X:\", X.shape)\n",
    "\n",
    "N = X.shape[0]\n",
    "D = X.shape[1]\n",
    "print(\"The number of data samples is N:\", N)\n",
    "print(\"The number of features is D:\", D)\n",
    "\n",
    "\n",
    "print(\"\\nShape of the labels vector y:\", y.shape)\n",
    "\n",
    "\n",
    "print(\"\\nFirst data sample in X:\", X[0,:])\n",
    "print(\"First label in y:\", y[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first exercise is to split the data into training and test sets. Let's set aside 80% of the data for training and 20% for testing. \n",
    "\n",
    "Your steps should be the following ones:\n",
    "1. Generate a vector of indices from 0 to N-1 (with N being the number of data samples) (Hint: you can use the function np.arange()\n",
    "2. Shuffle the indices (hint: you can use np.random.shuffle(). Look up how to use this function!)\n",
    "3. Select 80% of the indices. We have coded this for you but make sure you understand what these lines are doing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_ratio = 0.8\n",
    "\n",
    "# Step 1:\n",
    "indices = np.arange(N)\n",
    "\n",
    "# Step 2:\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "# Step 3:\n",
    "X_train    = X[indices[0:int(N*split_ratio)],:] \n",
    "y_train    = y[indices[0:int(N*split_ratio)]] \n",
    "# Split the test data using the remaining indices!\n",
    "X_test     = X[indices[int(N*split_ratio):],:]\n",
    "y_test     = y[indices[int(N*split_ratio):]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the shapes of `X_train`, `y_train`, `X_test`, `y_test`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(380, 12)\n",
      "(380,)\n",
      "(95, 12)\n",
      "(95,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "# The shapes should be (380, 12), (380,), (95, 12), (95,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After, we normalize the data such that each feature has zero mean and unit standard deviation. (Since you have not seen this in the lectures yet, we have provided the code for you. Study this well, we will be doing a lot of normalization during the exercise sessions!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Make mean 0 and std dev 1 of the data.\n",
    "'''\n",
    "def normalize(X):\n",
    "    mu    = np.mean(X,0,keepdims=True)\n",
    "    std   = np.std(X,0,keepdims=True) \n",
    "    X     = (X-mu)/std\n",
    "    return X, mu, std\n",
    "\n",
    "#Use train stats for normalizing test set\n",
    "X_train,mu_train,std_train = normalize(X_train)\n",
    "X_test = (X_test-mu_train)/std_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Attribute $X_4$ vs Price $y$')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Exploratory analysis of the data. Have a look at the distribution of prices vs features\n",
    "\n",
    "feature = 4\n",
    "plt.scatter(X_train[:,feature], y_train)\n",
    "plt.xlabel(f\"Attribute $X_{feature}$\")\n",
    "plt.ylabel(\"Price $y$\")\n",
    "plt.title(f\"Attribute $X_{feature}$ vs Price $y$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Q: Using the code above, explore the relation between different features and the house prices. Describe what you see. Can you identify any trends?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 Closed-form solution for linear regression\n",
    "\n",
    "\n",
    "The linear regression method has a closed form, analytical solution, as we have also seen in class.\n",
    "\n",
    "$$ \\mathbf{w^*} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y} $$\n",
    "\n",
    "\n",
    "Now let's code the analytical solution in the function `get_w_analytical` and to obtain the weight parameters $\\mathbf{w}$. Tip: You may want to use the function np.linalg.pinv(). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_w_analytical(X_train,y_train):\n",
    "    \"\"\"\n",
    "    compute the weight parameters w\n",
    "    \"\"\"\n",
    "        \n",
    "    # compute w via the analytical solution\n",
    "    w = np.linalg.inv(X_train.T @ X_train) @ X_train.T @ y_train\n",
    "    return w\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To assess our method's performance, we'll be using the mean squared error (MSE). \n",
    "\n",
    "$$ MSE = \\frac{1}{N}\\sum_{i=1}^N (\\hat{y}_i - y_i)^2$$\n",
    "\n",
    "where our prediction $\\hat{y}_i = \\mathbf{x}_i^T \\cdot \\mathbf{w} $.\n",
    "\n",
    "\n",
    "\n",
    "Let's code this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_loss(w, X_train, y_train,X_test,y_test):\n",
    "    def mse(X,y):\n",
    "        w = get_w_analytical(X,y)\n",
    "        return np.mean((X @ w - y)**2)\n",
    "    \n",
    "    loss_train = mse(X_train, y_train)\n",
    "    loss_test = mse(X_test, y_test)\n",
    "    print(\"The training loss is {}. The test loss is {}.\".format(loss_train, loss_test))\n",
    "    \n",
    "    return loss_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training loss is 434.4825923742592. The test loss is 360.6559653650676.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "360.6559653650676"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let's test our code!\n",
    "w_ana = get_w_analytical(X_train,y_train)\n",
    "get_loss(w_ana, X_train,y_train, X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q: What is the shape of the analytical weights?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.3. Adding a bias term"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The error of 400 is quite high! Note however that, in contrast to what we have seen in the lectures, we did not use any bias term $\\text{w}^{(0)}$. Let's see whether we can reduce the error by including a bias term.\n",
    "\n",
    "First, let's look more closely at what happens without a bias term. Formally, without a bias term, we are fitting a hyperplane that always passes through the origin. This is because our predictions can be expressed as\n",
    "$$ \\hat{y}_i = \\text{w}^{(1)}x_i^{(1)} +  \\text{w}^{(2)}x_i^{(2)} + ... + \\text{w}^{(12)}x_i^{(12)} $$\n",
    "\n",
    "or\n",
    "\n",
    "$$ \\hat{y}_i = \\mathbf{x}_i^T \\cdot \\mathbf{w} $$ \n",
    "\n",
    "Therefore, when $\\mathbf{x}_i=\\mathbf{0}$, $\\hat{y}_i= 0$, no matter what values $\\mathbf{w}$ takes. That's not ideal!\n",
    "\n",
    "Note: If you are confused about the transpose operation in $\\hat{y}_i = \\mathbf{x}_i^T \\cdot \\mathbf{w}$ above, here are the shapes of the matrices that are being multiplied:  \n",
    "* $\\mathbf{w}$ is DX1 \n",
    "* $\\mathbf{x}_i$ is DX1 (A reminder: The entire data $\\mathbf{X}$ is NXD, but when we select a single data sample from it, we express it as a column vector!) \n",
    "* The result $\\hat{y}_i$ is 1x1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Introducing the bias term:**\n",
    "        \n",
    "It would be a lot nicer if our predicted hyperplane didn't always have to pass through the origin. In math words:\n",
    "\n",
    "$$ \\hat{y}_i = \\text{w}^{(0)} + \\text{w}^{(1)}x_i^{(1)} +  \\text{w}^{(2)}x_i^{(2)} + ... + \\text{w}^{(12)}x_i^{(12)} $$\n",
    "\n",
    "Here, the $\\text{w}^{(0)}$ is the y-intercept. When $\\mathbf{x}_i=\\mathbf{0}$, $y_i= \\text{w}^{(0)}$. Neat!\n",
    "\n",
    "\n",
    "To handle this, we can add a column of 1s as a feature in our data $\\mathbf{X}$. This way, we could just say that the last feature $x_i^{(0)} = 1$ and\n",
    "\n",
    "$$ \\hat{y}_i = \\text{w}^{(0)}\\cdot 1 + \\text{w}^{(1)}x_i^{(1)} +  \\text{w}^{(2)}x_i^{(2)} + ... + \\text{w}^{(12)}x_i^{(12)} $$\n",
    "\n",
    "$$ \\hat{y}_i = \\text{w}^{(0)}x_i^{(0)} + \\text{w}^{(1)}x_i^{(1)} +  \\text{w}^{(2)}x_i^{(2)} + ... + \\text{w}^{(12)}x_i^{(12)} $$\n",
    "\n",
    "$$ \\hat{y}_i = \\mathbf{x}_i^T \\cdot \\mathbf{w} $$ \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "And we can keep using the same analytical solution formula as above! So by adding a column of 1s as the last feature of $\\mathbf{X}$, and running the analytical solution, we will find a $\\mathbf{w}$ with 13 features instead of 12. The last feature of the weights $\\mathbf{w}^{(0)}$ will be the bias term. This way, we wouldn't have to change any of the functions we wrote above.\n",
    "\n",
    "So let's get to it! Fill in the function below to append a bias term to the data matrices $\\mathbf{X}$. Your steps should be the following:\n",
    "1. Create a numpy array that is a column of 1s. It's shape should be NX1.\n",
    "2. Concatenate the ones column with the data matrix. Hint: use np.concatenate. Be careful what axis you specify!\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_bias_term(X_train):\n",
    "\n",
    "    ones_column = np.ones((X_train.shape[0],1))\n",
    "    X_train_bias = np.concatenate((X_train, ones_column), axis = 1)\n",
    "    return X_train_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training loss is 9.927412041848186. The test loss is 11.039240630718187.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "11.039240630718187"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_bias = append_bias_term(X_train)\n",
    "X_test_bias = append_bias_term(X_test)\n",
    "\n",
    "w_ana = get_w_analytical(X_train_bias,y_train)\n",
    "\n",
    "get_loss(w_ana, X_train_bias,y_train, X_test_bias,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now your loss should be around 10. That's much better, no?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.4. Solution using gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The linear regression model has an analytical solution, but we can also get the weight parameters $\\mathbf{w}$ numerically, e.g., via gradient descent. We will be using this approach to complete the function `get_w_numerical` below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let us code the gradient of the MSE loss, as we saw in class:\n",
    "$$\\nabla R = \\frac{2}{N}\\sum_i^N(\\mathbf{x}_i^T \\mathbf{w}-y_i)\\cdot \\mathbf{x}_i$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_gradient(X, y, w):\n",
    "    \"\"\"computes the gradient of the empirical risk, R\"\"\"\n",
    "\n",
    "    N = X.shape[0]\n",
    "    \n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    grad = 2 * np.mean( (X @ w - y) @ X)\n",
    "    return grad\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "()\n"
     ]
    }
   ],
   "source": [
    "#Let's create a random w to test the function above.\n",
    "w = np.random.normal(0, 1e-1, X_train_bias.shape[1])\n",
    "grad = find_gradient(X_train_bias, y_train, w)\n",
    "print(grad.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now, we can write the function that finds $\\mathbf{w}$ using gradient descent. Recall that gradient descent works via the update\n",
    "$$w_{k} \\leftarrow w_{k-1} - \\eta \\nabla R$$\n",
    "\n",
    "where $\\eta$ is the learning rate and $k$ is the iteration number.\n",
    "\n",
    "Fill in the function below to update $\\mathbf{w}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_w_numerical(X_train,y_train,X_test,y_test,epochs,lr):\n",
    "    \"\"\"compute the weight parameters w\"\"\"\n",
    "    \n",
    "    # initialize the weights\n",
    "    w = np.random.normal(0, 1e-1, X_train.shape[1])\n",
    "    \n",
    "    # iterate a given number of epochs over the training data\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        w -= lr * find_gradient(X_train, y_train, w)\n",
    "       \n",
    "        # Test every 500 epochs to see whether the training loss and test losses are going down.\n",
    "        if epoch % 500 == 0:\n",
    "            print(f\"\\nEpoch {1000+epoch}/{epochs}\")\n",
    "            get_loss(w, X_train,y_train, X_test,y_test)\n",
    "            \n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1000/15000\n",
      "The training loss is 9.927412041848186. The test loss is 11.039240630718187.\n",
      "\n",
      "Epoch 1500/15000\n",
      "The training loss is 9.927412041848186. The test loss is 11.039240630718187.\n",
      "\n",
      "Epoch 2000/15000\n",
      "The training loss is 9.927412041848186. The test loss is 11.039240630718187.\n",
      "\n",
      "Epoch 2500/15000\n",
      "The training loss is 9.927412041848186. The test loss is 11.039240630718187.\n",
      "\n",
      "Epoch 3000/15000\n",
      "The training loss is 9.927412041848186. The test loss is 11.039240630718187.\n",
      "\n",
      "Epoch 3500/15000\n",
      "The training loss is 9.927412041848186. The test loss is 11.039240630718187.\n",
      "\n",
      "Epoch 4000/15000\n",
      "The training loss is 9.927412041848186. The test loss is 11.039240630718187.\n",
      "\n",
      "Epoch 4500/15000\n",
      "The training loss is 9.927412041848186. The test loss is 11.039240630718187.\n",
      "\n",
      "Epoch 5000/15000\n",
      "The training loss is 9.927412041848186. The test loss is 11.039240630718187.\n",
      "\n",
      "Epoch 5500/15000\n",
      "The training loss is 9.927412041848186. The test loss is 11.039240630718187.\n",
      "\n",
      "Epoch 6000/15000\n",
      "The training loss is 9.927412041848186. The test loss is 11.039240630718187.\n",
      "\n",
      "Epoch 6500/15000\n",
      "The training loss is 9.927412041848186. The test loss is 11.039240630718187.\n",
      "\n",
      "Epoch 7000/15000\n",
      "The training loss is 9.927412041848186. The test loss is 11.039240630718187.\n",
      "\n",
      "Epoch 7500/15000\n",
      "The training loss is 9.927412041848186. The test loss is 11.039240630718187.\n",
      "\n",
      "Epoch 8000/15000\n",
      "The training loss is 9.927412041848186. The test loss is 11.039240630718187.\n",
      "\n",
      "Epoch 8500/15000\n",
      "The training loss is 9.927412041848186. The test loss is 11.039240630718187.\n",
      "\n",
      "Epoch 9000/15000\n",
      "The training loss is 9.927412041848186. The test loss is 11.039240630718187.\n",
      "\n",
      "Epoch 9500/15000\n",
      "The training loss is 9.927412041848186. The test loss is 11.039240630718187.\n",
      "\n",
      "Epoch 10000/15000\n",
      "The training loss is 9.927412041848186. The test loss is 11.039240630718187.\n",
      "\n",
      "Epoch 10500/15000\n",
      "The training loss is 9.927412041848186. The test loss is 11.039240630718187.\n",
      "\n",
      "Epoch 11000/15000\n",
      "The training loss is 9.927412041848186. The test loss is 11.039240630718187.\n",
      "\n",
      "Epoch 11500/15000\n",
      "The training loss is 9.927412041848186. The test loss is 11.039240630718187.\n",
      "\n",
      "Epoch 12000/15000\n",
      "The training loss is 9.927412041848186. The test loss is 11.039240630718187.\n",
      "\n",
      "Epoch 12500/15000\n",
      "The training loss is 9.927412041848186. The test loss is 11.039240630718187.\n",
      "\n",
      "Epoch 13000/15000\n",
      "The training loss is 9.927412041848186. The test loss is 11.039240630718187.\n",
      "\n",
      "Epoch 13500/15000\n",
      "The training loss is 9.927412041848186. The test loss is 11.039240630718187.\n",
      "\n",
      "Epoch 14000/15000\n",
      "The training loss is 9.927412041848186. The test loss is 11.039240630718187.\n",
      "\n",
      "Epoch 14500/15000\n",
      "The training loss is 9.927412041848186. The test loss is 11.039240630718187.\n",
      "\n",
      "Epoch 15000/15000\n",
      "The training loss is 9.927412041848186. The test loss is 11.039240630718187.\n",
      "\n",
      "Epoch 15500/15000\n",
      "The training loss is 9.927412041848186. The test loss is 11.039240630718187.\n"
     ]
    }
   ],
   "source": [
    "# compute w and calculate its performance\n",
    "w_num = get_w_numerical(X_train_bias,y_train,X_test_bias,y_test,15000,1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If everything went well, then your loss should be going down with each epoch!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Q: How do these results compare to those of the analytical solution?**\n",
    "\n",
    "**Q: In which cases may it be preferable to use the numerical approach over the analytical solution?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.5 Using sklearn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use the sklearn implementation of the linear regression model. sklearn is a library that contains implementations of many popular machine learning models, including the linear regression model!\n",
    "\n",
    "Please look up the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) to \n",
    "\n",
    "1. instantiate the LinearRegression model\n",
    "2. fit the model to our training data\n",
    "3. evaluate the model on the test data\n",
    "4. and compare the results with our previous outcomes\n",
    "\n",
    "Especially check out the example code they provide!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE of sklearn linear regression model on test data:  13.83670510143467\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import metrics\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "lr.score(X_test, y_test)\n",
    "y_hat = lr.predict(X_test)\n",
    "\n",
    "print('MSE of sklearn linear regression model on test data: ' , metrics.mean_squared_error(y_test,y_hat))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

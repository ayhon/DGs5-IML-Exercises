{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise Session: Logistic Regression\n",
    "$\\renewcommand{\\real}{\\mathbb{R}}$\n",
    "$\\renewcommand{\\xb}{\\mathbf{x}}$\n",
    "$\\renewcommand{\\wb}{\\mathbf{w}}$\n",
    "$\\renewcommand{\\Xb}{\\mathbf{X}}$\n",
    "$\\renewcommand{\\yb}{\\mathbf{y}}$\n",
    "$\\renewcommand{\\Yb}{\\mathbf{Y}}$\n",
    "$\\DeclareMathOperator*{\\argmin}{argmin}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib notebook\n",
    "\n",
    "# project files\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import helpers as helpers\n",
    "\n",
    "# 3rd party\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Introduction\n",
    "\n",
    "This week's exercise is about linear classification, in particular, logistic regression. We will focus on the binary classification scenario, i.e., two classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Binary Classification with Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by loading the [_Iris Flower Dataset_](https://en.wikipedia.org/wiki/Iris_flower_data_set). To facilitate visualization, we will only use 2 out of the 4 features of this dataset. Furthermore, we will use 2 out of the 3 classes in this dataset, named *setosa* and *versicolor*. Therefore, for this part our dataset with two classes is as follows:\n",
    "\n",
    "  - data: $\\Xb \\in \\real^{N \\times 3}$, $\\forall \\xb_i \\in \\Xb: \\xb_i \\in \\real^{3}$ (2 features and the bias)\n",
    "  - labels: $\\yb \\in \\real^{N}$, $\\forall y_i \\in \\yb: y_i \\in \\{0, 1\\}$ \n",
    "\n",
    "Note that $\\Xb$ is a matrix of shape $(N \\times (D+1))$. However, a single data sample $\\xb_i$ is a column vector of shape $((D+1) \\times 1)$. \n",
    "To compute a scalar product of one data sample with the weight vector $\\wb$ (also a column vector of shape $((D+1) \\times 1)$), we write $\\xb_i^\\top\\cdot\\wb$. To perform a matrix-vector multiplication of the entire data matrix with the weight vector, we write $\\Xb\\cdot\\wb$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads the data and split them into training and test subsets.\n",
    "data, labels = helpers.load_ds_iris(sep_l=True, sep_w=True, pet_l=False, pet_w=False,\n",
    "                              setosa=True, versicolor=True, virginica=False, addbias=True)\n",
    "fig = helpers.scatter2d_multiclass(data, labels)\n",
    "\n",
    "num_samples = data.shape[0]\n",
    "\n",
    "fraction_train = 0.8\n",
    "np.random.seed(0)\n",
    "rinds = np.random.permutation(num_samples)\n",
    "\n",
    "data_train = data[rinds[:int(num_samples * fraction_train)]] \n",
    "labels_train = labels[rinds[:int(num_samples * fraction_train)]]  \n",
    "\n",
    "data_test = data[rinds[int(num_samples * fraction_train):]] \n",
    "labels_test = labels[rinds[int(num_samples * fraction_train):]]  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Short introduction\n",
    "\n",
    "In logistic regression, the probability (score) that a data point belongs to the positive class is expressed as\n",
    "$$P(y_i=1|\\xb_i, \\wb) = \\frac{1}{1+e^{-\\xb_i^{\\top}\\cdot \\wb}} $$\n",
    "\n",
    "This relies on the sigmoid function is defined as\n",
    "$$\\sigma(t)= \\frac{1}{1+e^{-t}}$$\n",
    "\n",
    "So in our case, the prediction of our model is defined as\n",
    "$$\\hat{y}(\\xb_i)=\\sigma(\\xb_i^{\\top}\\cdot \\wb)= \\frac{1}{1+e^{-\\xb_i^{\\top}\\cdot \\wb}}$$\n",
    "\n",
    "Let us code this function. You can use the numpy function `np.exp(x)` to take the exponential of a number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(t):\n",
    "    \"\"\" Sigmoid function\n",
    "    \n",
    "    Args:\n",
    "        t (np.array): Input data of shape (N, )\n",
    "        \n",
    "    Returns:\n",
    "        np.array: Probabilites of shape (N, ), where each value is in [0, 1].\n",
    "    \"\"\"\n",
    "    return ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that the cross entropy loss is defined as:\n",
    "$$\n",
    "R(\\wb) = -\\sum_i (y_i \\log(\\hat{y}(\\xb_i)) + (1-y_i)\\log(1-\\hat{y}(\\xb_i))) $$\n",
    "\n",
    "Let's code it using NumPy. If you do it correctly, it can be written in one line!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_logistic(data, labels, w): \n",
    "    \"\"\" Logistic regression loss function for binary classes\n",
    "    \n",
    "    Args:\n",
    "        data (np.array): Dataset of shape (N, D).\n",
    "        labels (np.array): Labels of shape (N, ).\n",
    "        w (np.array): Weights of logistic regression model of shape (D, )\n",
    "    Returns:\n",
    "        int: Loss of logistic regression.\n",
    "    \"\"\"    \n",
    "    return ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To move the weight vector towards the optimal weights, we need to compute the gradient of the loss function. This gradient is defined as\n",
    "$$\\nabla R(\\wb)= \\sum_i (\\hat{y}(\\xb_i) - y_i)\\xb_i $$\n",
    "Let us put this into a nice matrix format:\n",
    "$$\\nabla R(\\wb)= \\Xb^\\top(\\hat{y}(\\Xb) - \\yb) = \\Xb^\\top(\\sigma(\\Xb\\cdot \\wb) - \\yb),\n",
    "$$\n",
    "\n",
    "where $\\hat{y}(\\Xb) = \\sigma(\\Xb\\cdot \\wb)$ and $\\sigma(\\Xb\\cdot \\wb)$ computes the sigmoid for each data sample separately, and returns a vector of shape $(N \\times 1)$.\n",
    "\n",
    "Fill in the function for computing the gradient `gradient_logistic()`. You can use the numpy function `np.dot()` or an operator `@` for matrix multiplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_logistic(data, labels, w):\n",
    "    \"\"\" Logistic regression gradient function for binary classes\n",
    "    \n",
    "    Args:\n",
    "        data (np.array): Dataset of shape (N, D).\n",
    "        labels (np.array): Labels of shape (N, ).\n",
    "        w (np.array): Weights of logistic regression model of shape (D, )\n",
    "    Returns:\n",
    "        np. array: Gradient array of shape (D, )\n",
    "    \"\"\"\n",
    "    return ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Classification using a logistic regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us write a function to perform classification using logistic regression, `logistic_regression_classify()`. This function uses the weights we find during training to predict the labels for the data.\n",
    "\n",
    "**Hints:**\n",
    "* We classify our data according to $P(y_i=1|\\xb_i, \\wb)$. If the value of $P(y_i=1|\\xb_i, \\wb)$ is less than 0.5 then the data point is classified as label 0. If it is more than or equal to 0.5 then we classify the data point as label 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression_classify(data, w):\n",
    "    \"\"\" Classification function for binary class logistic regression. \n",
    "    \n",
    "    Args:\n",
    "        data (np.array): Dataset of shape (N, D).\n",
    "        w (np.array): Weights of logistic regression model of shape (D, )\n",
    "    Returns:\n",
    "        np.array: Label assignments of data of shape (N, )\n",
    "    \"\"\"\n",
    "    #### write your code here: find predictions and threshold.\n",
    "    predictions = ...\n",
    "     \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We measure the performance of our classifier with *accuracy* metric. It is defined as $$ f_{\\text{acc}} = \\frac{\\text{# correct predictions}}{\\text{# all predictions}}$$\n",
    "Implement the following `accuracy()` function using the predicted and ground truth labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(labels_gt, labels_pred):\n",
    "    \"\"\" Computes accuracy.\n",
    "    \n",
    "    Args:\n",
    "        labels_gt (np.array): GT labels of shape (N, ).\n",
    "        labels_pred (np.array): Predicted labels of shape (N, ).\n",
    "        \n",
    "    Returns:\n",
    "        float: Accuracy, in range [0, 1].\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    return ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Training a logistic regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the optimal weights for the given training data, we need to train our model. Fill in the missing parts of the function `logistic_regression_train()`.\n",
    "\n",
    "The function first initializes the weights randomly (according to a Gaussian distribution). In each iteration, you should compute the gradient using `gradient_logistic` and take a gradient step to update the weights. Given that $\\eta$ is the learning rate, recall that a gradient step is expressed as: $$ \\wb_{[t + 1]}  = \\wb_{[t]} - \\eta \\nabla R(\\wb_{[t]}) $$\n",
    "\n",
    "The `loss`, `plot` and `print_every` parameters affect the way the loss is printed and the predictions are displayed. You do not need to modify these parts.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression_train(data, labels, max_iters=10, lr=0.001, \n",
    "                              print_period=1000, plot_period=1000):\n",
    "    \"\"\" Training function for binary class logistic regression. \n",
    "    \n",
    "    Args:\n",
    "        data (np.array): Dataset of shape (N, D).\n",
    "        labels (np.array): Labels of shape (N, ).\n",
    "        max_iters (integer): Maximum number of iterations. Default:10\n",
    "        lr (integer): The learning rate of  the gradient step. Default:0.001\n",
    "        print_period (int): Num. iterations to print current loss. \n",
    "            If 0, never printed.\n",
    "        plot_period (int): Num. iterations to plot current predictions.\n",
    "            If 0, never plotted.\n",
    "    Returns:\n",
    "        np.array: weights of shape(D, )\n",
    "    \"\"\"\n",
    "\n",
    "    #initialize the weights randomly according to a Gaussian distribution\n",
    "    weights = np.random.normal(0., 0.1, [data.shape[1],])\n",
    "    for it in range(max_iters):\n",
    "        ########## write your code here: find gradient and do a gradient step\n",
    "        gradient = ...\n",
    "        weights = ...\n",
    "        ##################################\n",
    "        predictions = logistic_regression_classify(data, weights)\n",
    "        if print_period and it % print_period == 0:\n",
    "            print('loss at iteration', it, \":\", loss_logistic(data, labels, weights))\n",
    "        if plot_period and it % plot_period == 0:\n",
    "            fig = helpers.visualize_predictions(data=data, labels_gt=labels, labels_pred=predictions)\n",
    "            plt.title(\"iteration \"+ str(it))\n",
    "        if accuracy(labels, predictions) == 1:\n",
    "            break\n",
    "    fig = helpers.visualize_predictions(data=data, labels_gt=labels, labels_pred=predictions)\n",
    "    plt.title(\"iteration \"+ str(it))\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the code below to see your training in action. What do you observe? Try playing with the learning rate and number of max iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = logistic_regression_train(data_train, labels_train, max_iters=100000, lr=1e-2, print_period=1000, plot_period=3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = logistic_regression_classify(data_test, weights)\n",
    "fig = helpers.visualize_predictions(data=data_test, labels_gt=labels_test, labels_pred=predictions)\n",
    "plt.title(\"test result\")\n",
    "print(\"Accuracy is\", accuracy(labels_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Different Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now use the same dataset, with the same 2 features as before, but change the 2 classes to *versicolor* and *virginica*. As we use the features and the same number of classes, our dataset as the same form as before:\n",
    "\n",
    "  - data: $\\Xb \\in \\real^{N \\times 3}$, $\\forall \\xb_i \\in \\Xb: \\xb_i \\in \\real^{3}$ (2 features and the bias)\n",
    "  - labels: $\\yb \\in \\real^{N}$, $\\forall y_i \\in \\yb: y_i \\in \\{0, 1\\}$ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads the data and split them into trian and test subsets.\n",
    "data, labels = helpers.load_ds_iris(sep_l=True, sep_w=True, pet_l=False, pet_w=False,\n",
    "                              setosa=False, versicolor=True, virginica=True, addbias=True)\n",
    "fig = helpers.scatter2d_multiclass(data, labels)\n",
    "\n",
    "num_samples = data.shape[0]\n",
    "\n",
    "fraction_train = 0.8\n",
    "np.random.seed(0)\n",
    "rinds = np.random.permutation(num_samples)\n",
    "\n",
    "data_train = data[rinds[:int(num_samples * fraction_train)]] \n",
    "labels_train = labels[rinds[:int(num_samples * fraction_train)]]  \n",
    "labels_train[labels_train==2] = 0\n",
    "\n",
    "data_test = data[rinds[int(num_samples * fraction_train):]] \n",
    "labels_test = labels[rinds[int(num_samples * fraction_train):]]  \n",
    "labels_test[labels_test==2] = 0\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can perform training and evaluation on this dataset. We leave this part to you. \n",
    "Note: We recommend reducing the plotting period of the `logistic_regression_train` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train\n",
    "weights = ...\n",
    "#Predict\n",
    "predictions = ...\n",
    "\n",
    "fig = helpers.visualize_predictions(data=data_test, labels_gt=labels_test, labels_pred=predictions)\n",
    "plt.title(\"test result\")\n",
    "\n",
    "#Calculate accuracy\n",
    "test_accuracy = ...\n",
    "print(\"Test Accuracy is\", test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q: Comment on the accuracy. What is the difference between the dataset  in the first part and this one?**\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
